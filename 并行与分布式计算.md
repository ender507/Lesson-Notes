# 并行与分布式计算 
1~2章的笔记完全来源于课件（前期网课不留档），之后的笔记来自课堂和课件

## 1. 并行计算概览

### 1.1 并行计算的提出与趋势

可并行的计算问题应该满足： 

- 可分解成同时计算的几个离散片段
- 在任意时刻可同时执行多条指令
- 多计算资源所花的时间少于单计算资源

计算资源一般为： 

- 具有多处理器／多核的单台主机
- 通过网络连接的若干数量的主机

并行计算优势：

- 模拟、建模自然复杂并行现象
- 省时间和开销
- 解决更大更复杂问题
- 并发处理
- 利用非本地资源
- 更好地发挥底层并行硬件

用途：

- 科学和工程计算
- 工业和商业应用

驱动力：

- 应用发展趋势
- 架构发展趋势

VLSI大规模集成电路并行发展历程：

1. bit级的并行
2. 指令级的并行
3. 线程级的并行

未来属于异构、多核的SOC（system on chip）架构，即异构CPU、DRAM、GPU等集成到一个芯片上。

从硬件角度来讲，今天的单个计算机都是并行计算机，主要体现为： 

- 多个功能单元（L1 Cache、L2 Cache、Branch、Prefetch、GPU等）
- 多个执行单元或者核心
- 多个硬件线程

多个单独的计算机通过网络连接起来形成计算集群、

Moore’s law新解： 

- 每两年芯片上的核心数目会翻倍
- 时钟频率不再增加，甚至是降低
- 需要处理具有很多并发线程的系统
- 需要处理芯片内并行和芯片之间的并行
- 需要处理异构和各种规范（不是所有的核都相同）

有了能并行的硬件，还要实现能并行的软件。编译器将软件并行化的难度太大，不如让程序员学习并行编程。

并行程序的考虑点：

- 任务并行：计算密集型任务，可拆分成不同的子任务
- 数据并行：数据密集型任务，每个核心运行相同程序

### 1.2 Amdahl’s Law

一般来说，Speed Up = 单线程运行时间 / 多线程运行时间

Amdahl’s Law：`SpeedUp = 1/((1-p)+p/n)`，其中p为可并行部分的比例，n为线程数。

应用程序只有一部分可以并行，大量的串行代码降低并行性能

-----------

## 2. 并行架构

### 2.1 Flynn’s Taxonomy分类法

- SISD：单核计算机
- MISD：无常见系统
- SIMD：向量处理器，GPU
- MIMD：现代并行系统

### 2.2 单处理器并行

#### 2.2.1 执行指令的并行

考虑指令集并行ILP，如：

- 流水线
  - 增加带宽但不改变延迟
  - 带宽为流水线最慢的步骤限制
  - 潜在的speed up：流水线步骤数
  - 需要很多锁存器
  - 带来了冒险：
    - 结构冒险：竞争相同硬件
    - 数据冒险：使用的数据在之前的指令改变且没处理完
    - 控制冒险：分支跳转的判断与取指令
- 超标量：一次启动多条指令
- VLIW：编译器决定哪些操作并行执行
- 向量处理器：特别的操作分组并行执行
- 乱序执行OOO（out of order）

SIMD/Vector Processing:

- GPU：CUDA、OpenCL
- CPU：ISPC(Intel SPMD Program Compiler)

多线程/线程级并行TLP：

共享内容存在堆中，私有的在栈中。

TLP和ILP结合：同步多线程Simultaneous Multithreading/超线程Hyperthreading

#### 2.2.2 内存系统

局部性原理：

- 空间局部性
- 时间局部性

存储分层结构：

- 寄存器 -> 片上缓存 -> 二级缓存（SRAM）-> 主存（DRAM）-> 辅存(硬盘) -> 三级存储（磁带）

- CPU 寄存器 -> CPU缓存 ->物理内存 -> 固态内存 -> 虚拟内存

内存带宽由内存总线带宽和内存单元决定。

### 2.3 多核芯片

并行芯片拓展处理器

嵌入式并行处理器：

- 暴露内存分层和互连网络，通过编程获得最佳性能。跨平台不那么重要
- 定制同步机制
  - 互锁通信通道(如果数据没有准备好，读取时的处理器块)
  - 障碍信号
  - 特殊的原子操作单元

GPU

### 2.4 并行架构

即多处理器机器。

MIMD机器：

- 共享内存
- 消息传递

-------

## 3. 并行编程模型

- 共享地址空间
- 消息传递
- 数据并行（通过上述二者实现）

先进的并行编程模型与并行机器相分离。

### 3.1 共享地址

- 通信隐式地包含于存取数据中
- 没有数据所有权的概念，便于编程
- 难以理解和管理数据局部性

地址可以是一致的，也可以是不一致的（NUMA架构）

GPGPU编程模型（数据并行）

### 3.2 线程模型（本质是共享地址）

- 数据拆分
- 任务拆分（静态/动态调度）

不会因为多线程任务而出错的函数称为线程安全函数，可以被中断后继续执行的函数称为可重入函数。线程安全函数包含可重入函数。可重入函数不使用全局变量、不进行I/O操作。

粒度过小容易造成工作负载加重，太大造成负载不均衡

### 3.3 消息传递模型

通过显示的I/O操作进行交流

### 3.4 GPGPU编程模型（数据并行模型）

#### 3.4.1 CUDA编程模型

CUDA编程模型的目标为SIMD编程。CUDA是对GPU的抽象。GPU内有多个Block，数个Block可以组成一个Grid（大小自定），Block中又由多个Thread构成。一个Grid内共享一个Global Memory、Constant Memory、Texture Memory，一个Block内共享一个Shared Memory，每个Thread有自己的Local Memory和寄存器。

内层的多层结构有利于利用程序的局部性原理，以此提高效率。

### 3.4.2 OpenCL编程模型

OpenCL是在异构处理器（CPU、GPU、DSP、FPGA等）上写程序的框架。抽象方式和CUDA相似。

OpenCL的数据并行是通过工作组work-group中的工作单元实现，同样也支持任务并行。

OpenCL最粗的粒度为主机Host，其次是计算设备Compute Device，相当于计算卡级别的粒度。再其次是计算单元Compute Unit，如核心，再在其中还有处理单元Processing Element。

主机有主存Host Memory，所有粒度都可以访问，每个设备上有自己的Global/Constant Memory。在计算设备内将数个计算单元划为一个工作组Workgroup，每个工作组内共享自己的Local Memory。内部的Work-item （可类比为Thread）有自己的Private Memory。

工作组内的work-item可以通过共享内存通信，但不同工作组的work-item不能直接通信。

OpenCL由FIFO的事件队列组织工作。

OpenCL的数据并行分为：

- 显示的SIMD
  - 自己定义kernel的指令流
  - 向量计算的组织要显示声明
  - 向量并行大小
  - 任务并行和任务到硬件的映射要声明
- 隐式的SIMD
  - 代码自动转换为支持并行
  - 隐式地决定向量类型
  - 隐式映射

#### 3.4.3 数据并行系统

数据流架构，非冯诺依曼体系，不含程序代码，只有数据。依据数据类型决定操作，送往相关的处理单元。处理单元高度专门化，从而提速。所以指令运行的顺序是不可预测的。

如：

```c
if(a>=0)a*=2;
else a=-a;
```

对于取反和翻倍，在硬件上设计专门的PE，由此上下两条语句可以并行，前提是送入的数据是不同种类的。

数据并行很少见，较为专门化。如`Maspar MP`架构。

数据流机器：

- 静态数据流机器：用传统的内存地址表示数据位置
- 动态数据流机器：内容编值内存CAM：用数据取地址而不是用地址取数据

#### 3.4.4 `IBM TrueNorth`

非冯诺依曼架构。模仿人的大脑设计的芯片，有多个神经元，将计算和存储单元相结合。

大胆的尝试，目前用处不大。

---------

## 4. 并行编程方法论

### 4.1 回顾和举例

并行编程模型：

- 共享地址空间

- 消息传递

- 数据并行

实际中经常将不同的并行编程模型组合起来使用。

距离：洋流计算（粒子的模拟与相互影响）、星系演化（星体之间的引力影响）、黑洞图像的绘制、稠密矩阵向量乘法

### 4.2 增量式并行化Incremental Parallelization

开始为一个串行的程序，通过识别或理论分析找出并行部分，将串行模块用并行代替。当找不到或效率难以改善的时候结束。

### 4.3 Culler的设计方法论

1. 拆分Decomposition：分解问题

2. 分派Assignment：将各个子问题划分到各个线程

3. 治理、编排Orchestration：协调好各个模块的通信

4. 映射Mapping：映射到硬件

#### 4.3.1 拆分

静态拆分：由编程人员将问题拆分成足够少的子任务以并行执行，要尽量减少各个任务之间的依赖，保证独立性。

动态拆分：

- 通过编译器识别依赖（部分数据依赖和控制依赖）
- 循环拆分优化

#### 4.3.2 分派

目标：负载均衡、减小通讯开销

哪些任务分配到同一线程，是静态分派，运行时具体分派到哪一线程属于动态分派。很多语言和框架直接实现了分派过程

#### 4.3.3 编排

涉及结构化通信（哪些线程之间要通信）、加同步、优化数据结构、合理调度任务

目标：减少通信和同步，尽可能保持同步性，从而减小开销

需要考虑机器能力

#### 4.3.4 映射

一般对程序员透明。由OS（如`pthread`）、编译器（如ISPC）或者直接由硬件进行映射（如CUDA映射到GPU）。

映射决策： 

- 相关线程放在同一处理器从而最大化利用局部性，减少同步开销和数据依赖

- 不相关线程放在同一处理器从而减小资源争用

### 4.4 Foster的设计方法论

1. 拆分Partition
2. 通信Communication
3. 归并、组合Agglomeration

4. 映射Mapping

#### 4.4.1 拆分

- 数据分解（域分解）：如找数组中的最大值
- 任务分解：如GUI功能
- 流水线并行（流水线本质也是任务分解）

Check List：

- 分解后的任务数量一般至少是处理器个数的10倍

- 尽可能减少冗余的数据（方便任务的扩展）
- 子任务规模基本相同（便于负载均衡）

- 问题规模的增大能够使得子任务数量的增大

#### 4.4.2 通信

确定任务之间通信的拓扑结构（Task-Channel Graph），节点和箭头

- 局部通信：创建Channel
- 全局通信：同步全部线程等

Check List：

- 减小通信开销
- 任务之间的通信均衡（避免某部分变成瓶颈）
- 尽量将全局通信变为局部的

- 通信也尽量做到并行

- 任务的计算要并行

#### 4.4.3 整合、归并

将划分好的部分子任务重新合并，从而提高性能、维护可扩展性和简化编程

##### 提高性能

- 合并需要传递数据的两个任务可以减少通信
- 将两个信息发送方和两个任务接受方结合可以减少通信开销（2次变为1次）

##### 可扩展性

将合理合并问题使得底下的资源能够充分利用（如问题数和核心数基本对等）

##### 减少编码量

合并子问题减少传送数据的考虑

##### Check List

- 提高程序局部并行性
- 相比原来的数据传输更快

- 不影响可扩展性
- 归并后的计算量和通信量也要均衡
- 问题规模增加，子任务数量也要增加
- 任务数量和系统硬件相匹配
- 平衡归并和代码修改的代价（便于维护）

#### 4.4.4 映射

- 单节点处理器：OS实现
- 分布式：用户实现

矛盾点：最大的处理器利用率和最小任务通信

映射决策树：

- 静态任务数量
  - 结构式通信（通信有拓扑结构）
    - 计算时长稳定：归并任务来减小开销（一个处理器一个任务）
    - 计算时长不稳定：循环地往CPU映射（先用CPU1，再用CPU2....以均衡任务）
  - 无结构通信：静态负载均衡方式
- 动态任务数量
  - 频繁的通信：动态负载平衡
  - 短时任务：运行时调度算法（需要调度器参与）

Checklist：

- 单个处理器的负载（一个还是多个任务）
- 静态还是动态的分配
- 如果有调度器，保证调度器不会成为瓶颈
- 如果是静态的，任务：处理器一般至少为10：1

### 4.5 依赖图Dependency Graph

节点和边。边表示依赖（数据依赖或控制依赖），节点表示数值或操作或函数调用

---------

## 5. OpenMP编程

OpenMP（open multi-processing，开放多处理）是共享内存模型的并行编程模型。

- UMA（单一内存访问）结构：所有CPU共享同一内存。内存地址相同但会有争用
- NUMA结构：CPU共享内存的同时还有自己的内存

OMP实现的并行本质是线程的并行，所以基本调度单位也是线程。OMP是显示的并行，需要自己定义线程。

fork-join模型：主线程创建子线程，子线程进行同步

整个OMP包含：

- 编译器
- 运行时的库
- 环境变量

### 5.1 编译器指令语法

#### 5.1.1 编译指令

`gcc -fopenmp`

#### 5.1.2 声明clause

`#include<omp.h>`

`#pragma omp <directive-name> [clause, ...]`

| 声明                           | 说明                                                         |
| ------------------------------ | ------------------------------------------------------------ |
| parallel                       | 自动并行化处理以下代码（每个线程都运行一遍以下代码块而不会分块），适用SPMD。parallel声明后一下的并行声明才能生效 |
| for                            | 分块执行for循环（迭代器可算、不中途跳出）                    |
| private (\<variable list>)     | 声明进程的私有变量（每个进程都有自己的），参数用逗号分隔。必须在并行模块外定义，**在并行模块内初始化**。并行模块不会影响外部的变量值。 |
| shared (\<variable list>)      | 声明共享变量                                                 |
| firstprivate(\<variable list>) | 在模块前初始化。即线程创建时会按共享变量的值赋初值           |
| lastprivate(\<variable list>)  | 将逻辑上最后运行完的并行模块的变量赋给共享变量(如for的最后一块) |
| num_threads(n)                 | 限制线程数                                                   |
| master                         | 只能主线程执行，约等于single nowait(区别在于执行的是否是主线程)                          |
| single                         | 只能串行执行                                                 |
| sections                       | 将下个代码块分成多个子块，每个子块并行执行                   |
| section                        | 声明sections下的一个子块                                     |
| reduction(op:list)             | 归并执行，op为操作(+,\*,-,&,\|,^,&&,\|\|八种)，list为参数，用冒号分隔。各个线程会创建私有变量，各个线程的结果通过op操作合并变量并赋值给全局变量 |
| barrier                        | 在并行模块内设置断点，进行阻塞等待                           |
| nowait                         | 不受到barrier限制，包括隐式的                                |
| critical                       | 声明临界区代码                                               |
| atomic                         | 声明原子操作，只支持++、+=等简单操作，只对单条语句有效       |
| schedule (type[, size])        | 控制迭代对线程的调度和分配。type为static表示静态分配（如，10次循环分配给2个线程，第一个线程总会执行前五次）。dynamic，每个进程每次取一次迭代，进程空闲后再去去下一次。size为一次分配的大小。guided类似于动态调度，但每次分配的循环次数不同，开始比较大，以后逐渐减小，size为最小值。 |

#### 5.1.3 库函数：

| 函数                       | 返回值                           | 参数列表 | 功能说明             |
| -------------------------- | -------------------------------- | -------- | -------------------- |
| omp_get_thread_num()       | 当前进程的进程号                 | -        | 返回进程号           |
| omp_get_num_threads()      | 进程数                           | -        | 返回进程总数         |
| omp_get_num_procs()        | 物理机器能用于并行处理的处理器数 | -        | 返回最大并行处理器数 |
| omp_set_num_threads(int t) |                                  | t:线程数 | 设置并行线程数       |

#### 5.1.4 补充

可移植操作系统接口POSIX是关注线程的而c/c++不关注。

相较于`cout`，`printf`用的更多。（`cout`的输出流`<<`会被视作多条指令而`printf`的内容一次输出）

关于private声明:

```c
int i=100;				
printf("%d\n",i);		//并行模块外，i为全局变量，值为100
#pragma omp parallel private(i)
{
    i = 200;			//并行模块内必须对private变量赋值，否则是无效值(firstprivate可以继承原来100的初值)
    printf("%d\n",i);	//i为局部变量，值为200
}
printf("%d\n",i);		//出并行模块后，i为全局变量，值为100(lastprivate可以继承并行模块200的赋值)
```

### 5.2 OMP总结

- 优势
  - 实现增量式的并行化
  - 适合数据解耦(decomposition)的程序
  - 跨平台
- 劣势
  - 不适合任务解耦的程序，因为各个任务之间的逻辑不同
  - 编译器不进行一些错误的检测，如死锁、数据竞争和pragma的基本语法

## 6. OpenMP的竞态和同步

### 6.1 同步

多线程调度执行的过程中控制读写顺序，所以需要同步。实现同步的方法有多种，如barrier、锁机制

在OMP中`#pargama omp barrier`可以实现。像`for`、`single`等声明隐式地含有同步，不需要额外声明。`nowait`可以无视`barrier`

- 互斥：可以看做一种特殊的同步，使用锁机制实现
- 临界区声明`critical`
- 锁（细粒度的同步）

### 6.2 数据竞争

小数据集数据竞争的概率更小。开发测试阶段难以检查数据竞争，运行时才好发现。

避免数据竞争：

- 变量私有化
  - `private`语句
  - 直接在线程函数声明变量
  - 在线程栈上分配空间
- 控制方式
  - 临界区、互斥与同步

互斥是一种特殊的同步。

`#pragma omp critical`能有效解决竞争，但内部是串行执行的，而且决定`critical`段完全有程序员决定。

`#pragma omp atomic`是特殊的一种临界区声明，原子声明的粒度很小，只能对特定内存进行操作，操作也简单。一般用于`++`、`--`或二值操作。如果`atomic`声明有效，一般来说比`critical`更快。

-----------

## 7. OpenMP的性能优化

加速比：串行运行时间/并行运行时间

效率：加速比/核心数量

Amdahl定律的不足：

- 忽略并行带来的开销，如进程创建、管理的开销；核心和线程变多带来了更多的通信开销

- 并行任务划分不均衡

优化原则：

1. 找到高效的**串行**算法

2. 最大化局部性（时间和空间）。经验原则：尽可能让不同的核心处理不同的数据对象，避免对cache的数据干扰

3. 调度优化：
   - 循环调度：
     - 静态调度：`for schedule (static,[chunk])`声明。chunk表示单个线程的迭代大小。使用轮询的方式分配任务。开销小、容易产生任务不均衡。适用于循环次数可预测、每次循环的工作量基本相等的任务
     - 动态调度：`for schedule (dynamic,[chunk])`，执行完成的线程请求任务。线程开销更高，但能有效解决负载不均衡。适用于循环次数不可预测的任务。
     - `for schedule (guided,[chunk])`，本质是动态调度。先给的迭代次数较多，之后慢慢减小。chunk为迭代数的最小值。  用于减小动态调度的线程开销
   - 循环变换
     - loop fission（裂变/分离）：循环中有依赖关系，拆分以取出不需要依赖的部分用于并行执行
     - loop fusion（合并）：融合循环增大粒度、减小线程开销
     - loop exchange：改变多个嵌套循环的先后次序，从而改变循环内的并行关系（如矩阵的遍历）、增大粒度、提升局部性

-------

## 8. MPI编程

Message Passing Interface

消息传递的编程模型的原则：

- 在逻辑上，消息传递的机器可以视作线程间的消息传递，每个进程有自己独立的地址空间

- 限制：每个数据处于不同的数据空间，需要显式地划分数据归属。线程拥有这份数据才能使用。（没有共享内存的共享变量概念）
- 同步（称为松散同步）和异步的传递方式
- SPMD模型

### 8.1 Send / Receive

- 阻塞：如果阻塞，操作未完成则不会进行下面的工作
- 缓冲：若无缓冲区，数据要等待发送和接受的同步，否则数据将丢失

发送和接受消息的方式分为三类：

- 无缓冲区阻塞式
  - 设计简单，强制send和receive的执行
  - 发送方只有得知接收方接收了才能执行下面的任务
  - 可能会产生空闲（发生和接受不同时）和死锁
- 有缓冲区阻塞式
  - 因为有缓冲区，将要发送的数据放入缓冲区发送方就能进行下面的任务，从而提升了性能
  - 缓冲区可以由软件或硬件（如网卡自带缓冲区）实现

- 非阻塞式（**必然有缓冲**）
  - 发生和接受的正确由程序员处理
  - 需要检查发送和接受是否正常执行
  - 提高性能



### 8.2 消息传递接口MPI

`mpicc`/`mpic++`、`mpirun`,`-n 5`参数为程序实例个数,`-f fileName`多节点运行，文件内写节点（点分十进制），每个节点要有自己的可执行程序文件、`mpiexec`

- `MPI_Init(int *argc,char *argv)`MPI开始的过程，传递`main`函数的入口参数
- `MPI_Finalize()`结束MPI过程

- `MPI_Comm_size(MPI_Comm comm, int *size)`查询通信子大小
- `MPI_Comm_rank(MPI_Comm comm, int *rank)`查询当前进程在通信子内的ID
- `MPI_Send(void *buf, int count, MPI_Datatype Datatype, int dest, int tag, MPI_Comm Comm)`，
- `MPI_Recv(void *buf, int count, MPI_Datatype Datatype, int source, int tag, MPI_Comm Comm, MPI_Status *status)`

MPI把互相通信的进程抽象为一个通信组，每个进程有自己的、在一个通信子内独立的编号rank。最大通信子为`MPI_COMM_WORLD`。`MPI_COMM_WORLD`包含所有进程，多个进程构成通信组，通信组和通信的上下文形成通信子communicator。一个进程可以属于多个通信组。

MPI的数据包含地址、数据数量、数据类型，加上编号tag成为消息。send和receive的tag匹配才能正常通信。source和tag可以不限定，填`MPI_ANY_SOURCE`或`MPI_ANY_TAG`即可

- `MPI_Send`标准模式（阻塞）
- `MPI_Bsend`缓冲模式
- `MPI_Ssend`同步模式：匹配到Receive就返回
- `MPI_Rsend`就绪模式：Receive已经Ready才返回

- `MPI_Isend`非阻塞，`Ibsend``Issend``Irsend`同理

### 8.3  死锁问题

- 双方都先发送再接受或反之
  - 合理安排发送和接收的顺序
  - 加缓存（隐式`Sendrecv`或显式`Bsend`）
  - 非阻塞式
- 发送方发生1、2两条消息而接收方先收2再收1等情况都会死锁

非阻塞式的`MPI_Isend`和`MPI_Irecv`最后需要加一项参数`MPI_Request *request`用于检查是否出错。除了自己看内容还能调用`MPI_Test(MPI_Request *request, int *flag, MPI_Status *status)`。MPI也支持同步点`MPI_Wait(MPI_Request *request, MPI_Status *status)`

`Sendrecv`是带buffer 的双向函数

### 8.4 群组通信和计算

- 通信子内同步`MPI_Battier(MPI_Comm comm)`
-  广播`MPI_Bcast(void*buf,int count, MPI_Datatype Datatype, int source,MPI_Comm comm)`
- 归并（多对一）`int MPI_Reduce(sendbuf,recvbuf,count,datatype,MPI_Op op,target,comm)`常用OP：`MPI_MAX`/`MPI_SUM`等

- `MPI_Allreduce`归并后把结果发回全部的进程
- `MPI_Scan`只对部分数据进行归并
- `MPI_Gather`从多个进程汇总数据
- `MPI_Allgather`汇总数据后发给所有进程
- `MPI_Scatter`将一组数据分散到多个进程（gather的逆过程）

- `MPI_Alltoall`

- `MPI_Group_*`和`MPI_Comm_*`控制通信组和通信子。如`MPI_Comm_split`将大的通信子拆成更小的。需要注意的是，改变了通信子则进程编号都会重排。

----------

## 9. MPI编程与OMP和MPI的联合编程

以矩阵和向量的乘积为例使用MPI进行编程。

1. 按行划分

`MPI_Allgatherv`使进程包含的数据量不一样。分配数据不均匀时可以使用。

2. 按列划分
3. 棋盘划分

`mpicc -fopenmp`

-------

## 10. GPGPU、CUDA和OpenCL

`nvcc`

### 10.1 英伟达GPU结构

CUDA GPU是流多处理器SM的集合，每个SM是SIMD流水线的集合。

- 流stream（指令序列）是一组顺序执行的网格grid

- grid包含多个快block，执行相同指令
- 线程块包含多个线程thread，即SM
- 每个线程相互独立，进行轻量的变量计算
- warp：指SIMD执行的最大带宽。一般是32个线程组成

每个线程有自己的控制流、PC、内存栈、寄存器，每个线程都能访问GPU的Global address。定位thread要先定位grid，再找block，再找thread

内存分层：

- thread级：private memory

- block级：shared memory
- grid级：global memory

### 10.2 OpenCL

SPMD：拷贝多个程序副本运行

---------

### 11. MapReduce

相较于MPI，MapReduce更加强调在更普通的PC机而不是高性能计算机的并行运算，更面向于数据计算型的应用，更加强调可靠性保障。

MapReduce的相关概念：

- 作业job：一个完整的程序，是多个Mapper和Reducer的集合，有一套数据集
- 任务task：一个Mapper和一个Reducer，以及作业的一部分
- 任务尝试task attempt：冗余运行的一次任务

如：要计算20个文本的单词总数，这是一个作业，将其分成20份，每一份就是一个任务。至少要运行20个任务尝试，如果机器出错还要重新运行从而增加任务尝试数（At Least Once策略）。

### 11.1 MapReduce编程模型

MapReduce将数据操作都抽象为`map()`和`reduce()`两个过程。前者本质是一个哈希的分散，后者是进行合并。Map映射时产生键值对，映射函数自己定义，Reduce按照定义将有同样键值的数据进行归并。

Map的中间结果会进行排序和归并，然后存入memory和disk，绝大部分的结果会存入本地磁盘。Reduce之前也会先收集所有的数据并行归并和排序，再进行reduce。reduce是向map节点主动请求数据（fetch）的，会产生空闲等待。

### 11.2 运行时环境

- 分区输入数据
- 调度算法：调度器Yarn，分配任务给各个节点。常用的是FIFO
- 处理机器失效：重新调度执行
- 处理通信：MapReduce通信完全透明

MapReduce是“计算跟着数据走”，和MPI相反。MapReduce更强调大数据而不是计算密集。

Map划分的数据大小以块block为基本单位，默认64MB。MapReduce会记住导致产生运算崩溃的值，下次运行时不运行。在大数据应用下有一定的合理性。

优化：reduce节点向map节点fetch数据产生忙等待的优化：

- 硬件优化
- 冗余执行任务，谁快用谁。
- reduce节点发起特定指令让map完的节点进行自己部分数据的本地归并

 MapReduce的优点：简化了并行编程的复杂性，包括网络、同步、数据划分、容错机制、负载平衡的透明化

----------

## 12. Spark编程

强调实时且不断的数据流和实时计算。Spark是面向数据流的编程框架，用数据流图表示计算过程并执行。编程简单、应用广泛（计算跟着数据走）、容易扩展到大规模集群上。

MapReduce对于multi-pass算法（反复进行数据迭代计算）比较低效（传统的MapReduce将数据存入硬盘），且没有高效的原语进行数据共享。Spark针对此进行了改进。

为了提升性能，Spark将数据存储在内存上，又考虑到内存的易失性，最终采用了弹性分布式数据集RDD技术（resilient distributed datasets）。RDD数据生成后只允许读不允许写，对数据的修改必须生成新的数据对象。

由于RDD数据只读，产生错误时可以从上游数据直接恢复然后继续计算，从而提高了容错性能。

-----------

## 13. 离散搜索与负载均衡

### 13.1 DFS

迭代+回溯算法，简单但是效率低。空间状态树每个节点有b个分支，深度为k，则复杂度为θ(b^k)。

如果有p=b^k个线程，考虑最开始k层串行执行，之后的层每一个子树用一个线程执行。若层数大于2k，串行部分的运算基本可以忽略不计。

若p!=b^k，前m层串行，之后并行。m越大则负载越均匀，但串行部分越多。

适用问题：

- 问题空间小
- 平均运行时间为多项式时间
- 可以在多项式时间内找到次优解

静态划分不能评估每个进程的工作量，导致负载不均衡。

- donor process：向其他进程发送任务的进程
- recipient process：请求和接受任务的节点
- half-split：均分
- cutoff depth（截断点）：不均分

划分策略：

- 将栈底的元素发送执行
- 发送截断位置的元素执行，适用于有检验规则、深度浅的问题
- 折中

动态负载平衡：

- 消息传递：从其他的进程窃取工作
- 共享地址空间：对工作加锁和解锁

基本分配算法：

- ARR（asynchronous round robin）：每个节点有一个指针
- GRR（global ~）整个系统只有一个计数器表示发往哪个进程
- RP（polling）随机查询：随机发任务(**随机的效率最好**)

### 13.2 分析性能

设W为全部串行的工作量，Wp为并行时一个进程的工作量，p为进程数，则并行的额外开销TO=p*Wp-W

额外开销：

- 通信
- 空闲时间
- 检测终止条件
- 共享资源的竞争
- 搜索本身的开销

定义搜索开销s=p*Wp/W，上界为p。此时并行的每个进程和串行工作量相当。此时p越小越好。若W>Wp则称为加速异常。

设t_comm为通信开销，V(p)为一个进程发送工作请求收到的工作量，则开销TO=t_comm\*V(p)\*logW，则销量E=1/(1+TO/W)

对于上面的分配算法。有：

- ARR:最坏情况V(p)=O(p^2)

- GRR:V（p）=p
- RP：最差情况V(p)无解，平均情况O(plogp)

### 13.3 BFS:最优优先搜索

已知最优结果，求从初始状态到最优结果的步骤

定义当前状态到结果状态的距离“曼哈顿距离”，各个方案中选择距离减小的方案。最差情况相当于广度优先搜索

并行解决方案：

- 单队列：通信开销很大，访问队列成为性能瓶颈，可扩展性差

- 多队列
  - 随机选择
  - 环形队列
  - 黑板通信机制：共享信息

------

## 14. 性能优化

关键目标：

- 负载均衡
- 减少通信
- 减少额外开销

### 14.1 负载均衡

调度方案：

- 静态调度：编译的时候确定调度方案，适用于任务大小可预测的场景
- 半静态：静态调度，可以临时改变调度方案，适用于短时间内好预测而长时间不好预测的任务
- 动态调度：运行时调度，适用于任务量不可估计时

### 14.2 Fork-join(线程的创建和同步)

用`Clik Plus`框架编写

`cilk_spawn``clik_sync`

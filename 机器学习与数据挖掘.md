# 机器学习与数据挖掘

## 零、 介绍

传统算法由程序和数据生成预测结果，而学习算法通过数据和学习算法成程序（训练），再由程序和数据生成预测结果（预测）。

机器学习分为：

- 监督学习：给定输入数据和对应的预测结果
- 无监督学习：只给定输入的数据
- 强化学习：给定动作序列和回报

本课主要介绍监督学习、无监督学习、数据挖掘应用

------

## 一、 线性回归

### 1.1 介绍

回归是基于给定的特征，对感兴趣的变量进行值的预测的过程。在数学上，回归的目的是建立从输入数值到监督数值的函数：
$$
\hat y=f(x_1,...,x_m)
$$
线性回归：限制函数为线性形式：
$$
f(x_1,...x_m)=w_0+w_1x_1+...+w_mx_m
$$
也就是找一组参数$\{w_k\}^m_{k=1}$，使得在训练集上，函数与预测值尽可能接近。

### 1.2 单特征情形

$$
f(x)=w_0+w_1x
$$

数学上，可以将问题化为将损失函数（代价函数）最小化：
$$
L(w_0,w_1)=\frac1n\sum^n_{i=1}(f(x^{(i)}-y^{(i)}))^2
$$
其中$x^{(i)}$和$y^{(i)}$分别表示第$i$个样本的特征值和目标值，$n$是训练集大小。

假定$w_0=0$，则最终可以求得：
$$
L(w_1)=\overline{x^2}w_1^2-2\overline{xy}w_1+\overline{y^2}
$$
显然是一个抛物线，很容易求得最低点，此时$w_1=\frac{\overline{xy}}{\overline{x^2}}$

加入$w_0$后，会得到一个二维的二次函数。可以采取令偏导数为0的方法来找最小值点。最终得到：
$$
w_0=\frac{\overline{xy}\overline{x}-\overline{x^2}\overline y}{\overline x^2-\overline{x^2}}\\
w_1=\frac{\overline{x}\overline{y}-\overline{xy}}{\overline x^2-\overline{x^2}}
$$

### 1.3 多特征情形

用标量形式处理较为麻烦，因此使用矩阵形势：
$$
f(x_1,...x_m)=w_0+w_1x_1+...+w_mx_m\\\to f(\bold x)=\bold x\bold w
$$
其中：
$$
\bold x = [1,x_1,x_2,...,x_m]\\
\bold w = [w_0,w_1,w_2,...,w_m]^T
$$
由此，损失函数表示为：
$$
L(\bold w)=\frac1n \sum^n_{i=1}(\bold x^{(i)}\bold w-y^{(i)})^2
$$
进一步可以写成：
$$
L(\bold w)=\frac1n||\bold X\bold w-\bold y||^2
$$
其中，$\bold X$是特征矩阵：
$$
\bold X\triangleq \left[\begin{matrix}\bold x^{(1)}\\...\\\bold x^{(n)}\end{matrix}\right]
$$
损失函数对向量$\bold w$求导得到：
$$
\frac{\part L(\bold w)}{\part \bold w}=\frac2n\bold X^T(\bold X\bold w-\bold y)
$$
因为损失函数是凹函数，因此可以令梯度为0得到最优点，即：
$$
\bold w^*=(\bold X^T\bold X)^{-1}\bold X^T\bold y
$$
由于导数为0，则相乘的两个张量垂直。$\bold X\bold w^*$在几何上可以看做是$\bold y$在$\bold X$张成的空间上的投影

### 1.4 数值优化

#### 1.4.1 梯度下降GD

可以通过解析得到的解通常不存在或计算过于昂贵，因此考虑使用一些数值方法，比如梯度下降：
$$
\bold w^{(t+1)}=\bold w^{(t)}-r\left.\frac{\part L(\bold w)}{\part\bold w}\right|_{\bold w=\bold w^{(t)}}
$$
其中$r$为学习率。通过适当的学习率，对模型参数进行迭代更新，最终可以收敛到最优点。如果学习速率过小，收敛速度会非常慢；如果学习率太大，迭代可能会发散，所以设置合适的学习速度是很重要的。

#### 1.4.2 随机梯度下降SGD

梯度下降算法每次迭代都需要计算训练数据集中所有数据样本的梯度，即$\bold w$，对于大型数据集来说，其复杂性将非常高。为了降低复杂度，可以使用数据集的一小部分来计算梯度，即小批量mini-batch。

获得mini-batch的方法：打乱、分段，设定`batchSize`，从而可以将数据分为第一个、第二个、...的`batch`，第`t`次迭代就用第`t`组的mini-batch。

#### 1.4.3 其他优化方法

这些方法往往比梯度下降收敛更快，但开销更大

- 牛顿法：使用二阶导数作学习率

- 拟牛顿方法

- 共轭梯度法
- 协调下降法

------

## 二、 线性分类器

### 2.1 二分类情形

线性回归的输出值与分类任务中的目标值不兼容。线性回归的结果范围为全体实数，而二分类中变量结果只有0和1。因此考虑使用sigmoid函数，也称logistic函数：
$$
\sigma(z)=\frac1{1+e^{-z}}
$$
逻辑回归的结果为：
$$
f(\bold x)=\sigma(\bold x\bold w)
$$
将值域变为了$[0,1]$。

损失函数为：
$$
L(\bold w)=(\sigma(\bold x\bold w)-y)^2
$$
等价于最小化如下函数：
$$
L(\bold w)=\left\{\begin{matrix}-log(\sigma(\bold{xw})),y=1\\
-log(\sigma(1-\bold{xw})),y=0\end{matrix}\right.
$$
从而能够写成交叉熵损失：
$$
L(\bold w)=-ylog(\sigma(\bold{xw}))-(1-y)log(\sigma(1-\bold{xw}))
$$
平方误差损失不是凹函数，而交叉熵损失是凹函数，因此后者更容易优化。

对交叉熵损失求梯度，得：
$$
\frac{\part L(\bold w)}{\part\bold w}=\frac1N\sum^N_{l=1}[\sigma(\bold {x}^{(l)}\bold w)-y^{(l)}]\bold x^{(lT)}
$$
解析解可能不存在或很难求，因此可以用上一章提到的数值方法计算。因为交叉熵是凹函数，因此梯度下降方法是可以收敛到最优点$\bold w^*$的。

进行预测时，由计算结果大于或小于0.5来进行分类，这等价于由$\bold{xw}$大于0或小于0来分类。决策边界为0。在所有边界上的$\bold x$形成了一个与$\bold w$正交的空间。这个空间在二维上表现为直线，三维上表现为平面。对于固定的向量$\bold w\in\R^K$，所有$\bold x\in\{\bold {xw}=0\}$构成了一个$K-1$维的超平面hyper-plane。超平面不能表示非线性边界，因此逻辑回归是线性分类。

### 2.2 多分类情形

一般方法：

- 一对多：将多类问题转化为多个二分类问题。对于K个类，需要K个二分类器，将某一类和其他类分开。选择函数值最大的一个分类器对应的类作为结果。

- softmax函数：直接将样本分类到某一个类中

$$
softmax_i(\bold z)-\frac{e^{z_i}}{\sum^K_{k=1}e^{z_k}}
$$

易见，所有类的softmax函数值之和为1。每一类的函数值就为它的概率。即：
$$
f_i(\bold x)=softmax_i(\bold{xW})=\frac{e^{\bold{xw_i}}}{\sum^K_{k=1}e^{\bold{xw_k}}}
$$
K=2时，softmax函数和logistic函数一致，参数为$\bold w_1-\bold w_2$

对于有K个类的训练数据集，其标号y用一个独热one-hot向量表示，即一个01串，**只有一位为1**，为1的那一位表示它是第几类。

目标是最大化应该被分到的类的概率，因此交叉熵损失函数可以写为：
$$
L(\bold w_1,\bold w_2,...,\bold w_K)=-\frac1N\sum^N_{l=1}\sum^K_{k=1}y_k^{(l)}\log softmax_k(\bold x^{(l)}\bold W)
$$
其中，$y_k^{(l)}$是第$k$个$y^{(l)}$的元素。梯度为：
$$
\frac{\part L(\bold w_1,\bold w_2,...,\bold w_K)}{\part\bold w_j}=\frac1N\sum^N_{l=1}(softmax_j(\bold x^{(l)}\bold W)-y_j^{(j)})\bold x^{(l)T}
$$
其中$\bold w_j,j=1,...,K$应该同步更新。

使用矩阵$\bold W=[\bold w_1,...\bold w_K]$，则：
$$
\frac{\part L(\bold W)}{\part\bold W}=\frac1N\sum^N_{l=1}\bold x^{(l)T}(softmax(\bold x^{(l)}\bold W)-\bold y^{(l)})
$$
其中，$softmax(\bold x^{(l)}\bold W)=[softmax_1(x^{(l)}\bold W),...,softmax_K(x^{(l)}\bold W)]$

更新：
$$
\bold W_{t+1}=\bold W_t-\left.r\frac{\part L(\bold W)}{\part \bold W}\right|_{\bold W=\bold W_t}
$$

------

### 三、 从概率角度来看回归与分类

### 3.1 介绍

回归和分类的目的是在给定输入数据x的情况下预测可能的输出y。这里的预测结果是由确定的函数给出的。而从概率的角度看，输出是在给定x的前提下得到的，因此要求的应该是条件概率$p(y|\bold x)$，然后使用一定的方法判断，如：

- 均值：$\hat y=\int yp(y|\bold x)dy$
- 最大后验概率MAP：$\hat y=arg\max_y(y|\bold x)$

### 3.2 回归

考虑多元高斯分布$p(\bold z)=\frac1{(2\pi)^{D/2}|\bold\Sigma|^{1/2}}\exp\{-\frac12(\bold z-\bold\mu)^T\bold\Sigma^{-1}(\bold z-\bold \mu)\}\triangleq N(\bold z;\bold \mu,\bold \Sigma)$，无论协方差如何，都可以保证高斯分布是单峰的，在$\bold \mu$取到。

每个协方差矩阵可以进行分解：$\bold \Sigma=\bold U\bold \Lambda\bold U^T$，其中$\bold U$为正交矩阵，即与自身的转置乘积为$\bold I$，$\bold \Lambda$为对角矩阵。令$\bold z'=\bold U\bold z$，$\bold\mu'=\bold U\bold \mu$，代入原高斯分布函数有：
$$
p(\bold z)=\frac1{(2\pi)^{D/2}|\bold\Sigma|^{1/2}}\exp\{-\frac12(\bold z'-\bold\mu')^T\bold\Sigma^{-1}(\bold z'-\bold \mu')\}
$$
从概率的角度出发，我们只需指定条件概率分布$p(ylx)$即可进行预测。对于回归，我们假设分布是正态分布，则有：
$$
p(y|\bold x;\bold w)=\frac1{\sqrt{2\pi\sigma^2}}\exp[-\frac12\frac{(y-\bold{xw})^2}{\sigma^2}]
$$
取对数得到：
$$
\log p(y|\bold x;\bold w)=-\frac12\frac{(y-\bold{xw})^2}{\sigma^2}+常数c
$$
使其最小化则使得$(y-\bold{xw})^2$最小化，这与回归中的损失函数是一致的。对于多特征情形，使用联合分布概率，同样也可以得到需要被最小化的函数为$L(\bold w)=\sum^N_{i=1}(y^{(i)}-\bold x^{(i)}\bold w )^2$，与回归中用的损失函数一致

从概率角度来看，线性回归等价于：

- 建模：假设条件分布属于高斯分布
- 训练：最小化对数似然函数

### 3.3 分类

#### 3.3.1 二分类

伯努利分布：$p(z)=\left\{\begin{matrix}\mu,z=1\\1-\mu,z=0\end{matrix}\right.$，其中$\mu\in[0,1]$表示结果为1的概率。也可以写成：$p(z)=\mu^z(1-\mu)^{1-z},z=0或1$

为了实现二值分类，假设条件概率为伯努利分布：
$$
p(y|bold x)=(\sigma(\bold {xw}))^y(1-\sigma(\bold {xw}))^{1-y}
$$
训练目的就是**最大化**对数似然函数，数值上等价于最小化交叉熵。

因此，二元分类等价于：

- 建模：假定输出为伯努利条件分布
- 训练：最大化对数似然函数

#### 3.3.2 多分类

分布为：$p(\bold z=onehot_k)=\mu_k$，其中$onehot_i$有多位，只有1位为1，表示第几类。

分布可以等价地写成：
$$
p(\bold z)=\Pi^K_{k=1}\mu_k^{z_k}
$$
其中$\bold z$是独热向量。

多分类相当于：

- 建模：
  - 设置概率为$\mu_k=softmax_k(\bold {xW})$
  - 假定条件概率分布为：$p(\bold y|\bold x)=\Pi^K_{k=1}[softmax_k(\bold{xW})]^{y_k}$
- 训练：给定训练样本，最大化似然函数

总之，这些分类和回归都相当于假定一个条件概率分布，然后用最大化对数似然函数的方法求解。

---------

## 四、 非线性建模、过拟合、标准化

### 4.1 非线性的模型

在输入和输出之间不具有线性关系的回归中、不具有线性决策边界的分类中需要非线性的模型。一个自然的想法就是提高数据次数。如，将单个$[x]$提高到$[x,x^2,x^3]$。在多特征情形下，对每个特征单独做变换，这个变化函数成为基函数：
$$
[x_1,...,x_m]\to[\phi_1(\bold x),...\phi_n(\bold x)]\triangleq \bold \phi(\bold x)
$$
如此一来，非线性模型就变为了：
$$
f(\bold x)=\bold \phi(\bold x)\bold w
$$
被称为基函数模型。它关于x是非线性的，但关于w还是线性的。优化模型结果变为了：
$$
\bold w^*=(\bold \Phi^T\bold\Phi)^{-1}\bold\Phi^T\bold y
$$
其中，
$$
\bold\Phi(\bold X)\triangleq\left[\begin{matrix}\bold\phi(\bold x^{(1)})\\...\\\bold\phi(\bold x^{(N)})\end{matrix}\right]
$$
回归和分类一样的结果变换前后的公式一样，只是$\bold x$被$\bold \phi$取代了。分类只能用数值方法求解。

### 4.2 过拟合

高维度可以更好地拟合训练数据，从拟合训练数据的角度来看，模型阶数越高，拟合效果越好。但高阶模型在测试数据上的表现可能较差。模型能够很好地处理不可见数据的能力称为模型的泛化能力。

每个模型都对应一定程度的复杂性，但是很难给出一个精确的表达式来描述模型的复杂性。一般来说，模型的复杂性取决于参数的数量，参数越多，模型越复杂。为了使模型表现良好，我们应该在模型的复杂性和表示能力之间取得平衡。

### 4.3 模型选择

选择模型应该选择在不可见的测试数据上表现最好的，而不能根据训练数据上的性能选择模型。不同类型的模型也可以进行比较。

验证集：留出一部分(训练数据的20% ~ 30%)作为验证集，剩余部分作为训练数据。训练集和验证集都不能包括测试数据。在训练集上训练模型，同时在验证集上评估模型，考虑在验证集中选择性能最好的模型。

如果验证集上误差随着模型复杂性的增加而减少，则表明模型欠拟合，否则是过拟合。

交叉验证：训练数据通常是很少的，如果留出很大一部分用于验证，则无法使用足够的训练数据。考虑使用折中的方案：K折交叉验证：将训练集分成K等份，每次用K-1份训练，剩下一份验证。对K中不同的可能都进行运算，最终使用在验证集上效果最好的一份。

赤池信息准则AIC：
$$
AIC=2M-2\log(L)
$$
其中M是参数个数，L是对数似然函数。

贝叶斯信息准则BIC：
$$
BIC=M\log N-2\log(L)
$$
其中N是训练数据数量。由于对数似然L的要求，这些准则只能用于概率模型中

### 4.4 标准化（正则化）

除了拟合训练数据外，还要对参数施加一些先验偏好，例如：
$$
\tilde L(\bold w)=L(\bold w)+\lambda||\bold w||_2^2
$$
其中$L(\bold w)$为回归或分类的原始损失，$||\bold w||_2^2=(\sum^K_{k=1}w_k^2)^\frac12$是$L_2$范式，$\lambda$是超参数。这种标准化被称为L2标准化。依据其性质，有：

- 让模型参数向0靠近
- $\lambda$越大，$\bold w $往往值更小

L1正则化也常用。与L2正则化相似，L1正则化也倾向于对模型参数取较小的值。但是L1正则化通常会导致$\bold w$的稀疏解，也就是说，$\bold w$中的很多元素都是0

---------

## 五、 支持向量机

### 5.1 线性分类的决策边界

如果数据是线性可分的，那么可能有多个不同的分类器，都能够使得损失为0。从逻辑回归中的超平面从最大限度地减少交叉熵损失的角度来看是最优的，理想分类器中的超平面从最小化误分类样本数量的角度来看是最优的，但它们都是在训练集上评估的。由此，即便是在训练集上性能相同，我们还是要判断哪个模型更优。

### 5.2 最大边际分类器

为了在看不见的数据上表现良好，直觉是要找到一个超平面，使边际尽可能大。这里的边际margin指的是分类超平面到其他各个类中样本的最近距离。如此一来，靠近该超平面的样本更有可能被正确分类。

为了表述边际，需要进行如下的定义。

每个$\bold x$可以被分解为：
$$
\bold x=\bold m_1+\bold m_2
$$
其中，$\bold m_1$是在超平面上的向量，即$\bold w^T\bold m_1+b=0$，而$\bold m_2$垂直于超平面，与$\bold w$平行。因此定义：
$$
h(\bold x)\triangleq \bold w^T\bold x+b=\bold w^T\bold m_2
$$
又因为$\bold m_2$与$\bold w$平行，因此还可以定义：
$$
\bold m_2=\gamma\cdot\frac{\bold w}{||\bold w||}
$$
其中，$|\gamma|$表示$\bold m_2$的长度。从而可以得到：
$$
\gamma=\frac{h(\bold x)}{||\bold w||}
$$
超平面另一侧的样本距离加个符号即可。如此一来，点$(\bold x,y)$到超平面的距离为：
$$
\frac{y\cdot h(\bold x)}{||\bold w||}=\frac{y\cdot(\bold w^T\bold x+b)}{||\bold w||}
$$
其中，$y\in\{-1,1\}$。数据集下的超平面边际由最小距离得到。则找最大边际分类器的参数，即找到参数$\bold w^*$和$b^*$使得边际最小：
$$
\bold w^*,b^*=arg\max_{\bold w,b}\{\frac1{||\bold w||}\min_l[y^{(l)}\cdot(\bold w^T\bold x^{(l)}+b)]\}
$$
但是没有相应的算法。考虑变换目标函数，即优化与原问题具有相同最优解的另一个目标函数。可以看到，$\bold w^*$和$b$同时增大k倍，也是一个最优解。所以一定存在最优解，使得：
$$
y^{(l)}\cdot(\bold kw^T\bold x^{(l)}+kb)\geq1 \forall l=1,2,...,n
$$
在所有的解中，有着最小的$||\bold w||^2$的一组就是要找的$\bold w^*,b^*$，从而将问题化为了：
$$
\min_{\bold w,b}\frac12||\bold w||^2s.t.:y^{(l)}\cdot(k\bold w^T\bold x^{(l)}+kb)\geq1 \forall l=1,2,...,n
$$
这是一个二次优化问题。用数值方法可以有效地找到其最优解。

### 5.3 软最大边际分类器

在之前的最大边界分类器中使用的假设训练样本是线性可分的，然而这样的超平面可能并不存在，即优化问题没有可行的解。因此可以弱化条件：
$$
y^{(n)}\cdot(\bold w^T\bold x^{(n)}+b)\geq1-\xi_n
$$
其中$\xi_n$大于等于0，称为松弛变量。则目标就变成了：
$$
\min_{\bold w,b,\bold \xi}\frac12||\bold w||^2+C\sum^N_{n=1}\xi_n
$$
其中，C用来控制相关的重要性。

### 5.4 支持向量机

到目前为止，最大边际分类器是线性的。为了使模型非线性化，我们可以通过基函数将原始特征x变换到另一个空间。问题变为了原始最大裕度优化问题：
$$
\min_{\bold w,b,\bold \xi}\frac12||\bold w||^2+C\sum^N_{n=1}\xi_n\\
s.t.:y^{(n)}\cdot(\bold w^T\bold \phi(\bold x^{(n)})+b)\geq1-\xi_n
$$
分类器为：
$$
\hat y(\bold x)=sign(\bold w^{*T}\phi(\bold x^{(n)})+b^*)
$$
从直观上看，数据在高维空间中更容易分离。为了获得更好的性能，我们希望映射后的x到更高维的空间。然而太高的话代价也很大。使用对偶形式方法解决时会要计算映射值的转置与自身的内积，导致高开销。这个问题可以通过使用内核技巧来解决。

核函数是一个二元函数，可以表示为某些函数的内积：
$$
k(\bold x,\bold x')=\phi(\bold x)^T\phi(\bold x')
$$
Mercer定理：如果函数$k(\bold x,\bold x')$是对称正定的，即：
$$
\int\int g(\bold x)k(\bold x,\bold y)g(\bold y)d\bold xd\bold y\geq0\forall g(\cdot)\in L^2
$$
就存在函数$\phi(\cdot)$使得$k(\bold x,\bold x')=\phi(\bold x)^T\phi(\bold x')$。一个函数如果满足正定条件就必然是核函数。

最常用的核函数之一是高斯核，有着无限维：
$$
k(\bold x,\bold x')=\exp\{-\frac1{2\sigma^2}||\bold x-\bold x'||^2\}
$$
利用核函数，可以将对偶最大边距分类器重写为：
$$
\max_{\bold a}g(\bold a)s.t.:a_n\geq0,a_n\leq C,\sum^N_{n=1}a_ny^{(n)}=0
$$
其中，
$$
g(\bold a)=\sum^N_{n=1}a_n-\frac12\sum^N_{n=1}\sum^N_{m=1}a_na_my^{(n)}y^{(m)}k(\bold x^{(n)},\bold x^{(m)})
$$
从而得到诱导分类器：
$$
\hat y(\bold x)=sign(\sum^N_{n=1}a_n^*y^{(n)})k(\bold x^{(n)},\bold x^{(m)})+b^*)
$$
核技巧：将函数k代入。如果$\phi$不改变$\bold x$则为线性最大边际分类器，否则为基于基函数的有限维非线性最大边际分类器，如果为高斯核，则为无限维非线性最大边际分类器。

### 5.5 与逻辑回归的关系

逻辑回归、理想分类器、线性最大边际分类器、软线性最大边际分类器这四个分类器可以在同一个框架下表述，唯一的区别在于所选择的损失函数的不同。

- 逻辑回归的损失函数：$L(\bold w,b)=-\sum^N_{n-1}E_{LR}(y^{(n)}h^{(n)})+\lambda||\bold w||^2$，其中$E_{LR}(z)=\log(1+\exp(-z))$
- 理想分类器中的损失函数：$L(\bold w,b)=\sum^N_{n=1}E_{Ideal}(y^{(n)}h^{(n)})+\lambda||\bold w||^2$，其中$E_{Ideal}(z)=0\quad if\quad z\geq0;1\quad otherwise$
- 线性最大边际分类器为：$L(\bold w,b)=\sum^N_{n=1}E_\infin (y^{(n)}h^{(n)}-1)+\frac12||\bold w||^2$，其中$E_\infin(z)=0\quad if\quad z\geq0;+\infin\quad otherwise$
- 软线性最大边际分类器：$L(\bold w,b)=\sum^N_{n=1}E_{SV}(y^{(n)}h^{(n)})+\lambda||\bold w||^2$，其中$E_{SV}(z)=\max(0,1-z)$，又被称为hinge loss

其中，$h^{(n)}=\bold w^T\bold x^{(n)}+b$，$y^{(n)}=\pm1$

-------

## 六、 神经网络

### 6.1 神经网络的介绍

动机：在不同应用中实现非线性模型。为了增加灵活性，我们将之前的基函数改为可以学习的。可以将变换看做一组函数：
$$
\phi_i(\bold x)=a(\sum^m_{l=1}w_{il}'x_l)
$$
其中$a$称为激活函数。该函数不能省略，否则输出是与$\bold x$线性相关的。常用的激活函数有：sigmoid、tanh、ReLU、Leaky ReLU

### 6.2 向后传播

在神经网络中，我们需要所有参数的梯度，从而需要一层层往前求导。依据链式法则，可以省略很多的求导步骤。

### 6.3 典型的神经网络

#### 6.3.1 CNN卷积神经网络

全连接网络网络的弱化(也称为多层感知(MLP))。参数数量大，容易导致过拟合。CNN常常利用空间结构（例如:在图像/视觉数据中）的平移不变性进行学习预测

与MLP的区别：

- 局部连接：每个神经元只与前一层的一部分神经元相连
- 参数共享：不同神经元的参数是相连的

卷积处理后的图像称为特征图或激活图。不同的滤波器提取图像中不同的特征。

池化：

- 降低维数
- 引入非线性
- 保持一定程度的空间不变性
- 池化在所有层上都是非必要的

#### 6.3.2 RNN循环神经网络

$$
h_t=f_h(h_{t-1},x_t)\\
y_t=f_o(h_t)
$$

每次的输出不仅与本次的输入有关，还与之前的输入有关。

-------

## 七、 优化和训练技巧

### 7.1 优化算法

#### 7.1.1 随机梯度下降法SGD

求损失函数的梯度时需要对所有参数求梯度（求梯度的对象可能是向量或矩阵），每次随机选择其中一个参数计算梯度而不是全部的从而提高效率。随机梯度下降很容易卡在局部最优点，由于神经网络的病理曲率，收敛速度较慢，但效率更快。

#### 7.1.2 SGD+momentum

$$
\bold v_t=\rho \bold v_{t-1}+\nabla f(\bold w_t)\\
\bold w_{t+1}=\bold w-lr*\bold v_t
$$

可以看做从山顶往山脚走的时候增加了摩擦力的影响。

#### 7.1.3 RMSProp

$$
\bold s_t=\rho \bold s_{t-1}+()1-\rho\nabla f^2(\bold w_t)\\
\bold w_{t+1}=\bold w-lr*\nabla f(\bold w_t)⊘\sqrt{\bold s_t}
$$

其中⊘  表示元素除法。它将不同维度的梯度重新缩放到同一水平，缓解了病理性曲率问题。

#### 7.1.4 Adam

$$
\bold m_t=\beta_1*\bold m_{t-1}+(1-\beta_1)\nabla f(\bold w^t)\\
\bold s_t=\rho \bold s_{t-1}+()1-\rho\nabla f^2(\bold w_t)\\
\bold w_{t+1}=\bold w-lr*\bold m_t⊘\sqrt{\bold s_t}
$$

$\bold m_t$是之前的梯度的移动平均。一般$\beta_1=0.9,\beta_2=0.999,lr=10^{-3}$。$\beta_1=0$时与RMSProp相同。

### 7.2 训练技巧

#### 7.2.1 预处理

- 数据中心化：数据都减去均值
- 变量标准化
- 变量白化

显然，成本函数在不同的维度以不同的速率下降，这导致梯度下降的收敛速度较慢。如果对特征进行类似尺度的预处理，则可以改善。

#### 7.2.2 超参数调优

为了给这些超参数设置合适的值，我们需要将整个数据集分为三个部分，即训练集、验证集和测试集。首先尝试一些在许多任务中执行良好的默认值，根据训练过程中观察到的现象选择数值。

------------

## 八、 决策树

### 8.1 介绍

给定数据集，我们希望从基于输入的多种属性对结果进行预测。与分类器不同的是，决策树通过构建一棵属性的树，将数据分类成不同的类别。中间节点表示属性，叶子节点表示预测结果，边表示属性值。

构建决策树的时候需要考虑：

- 每个节点拓展的是什么属性
- 如何划分属性值从而产生子树
- 什么时候停止拓展

### 8.2 选择拓展属性的标准

在所有属性中，选择拓展的属性应当是拓展后获得信息更多的那一个属性，也就是说，通过拓展这个属性，我们能更好地对区分的样本进行分类。为了度量各个属性包含的信息量，考虑测量随机变量的不确定性。不确定性在数学上用熵来描述。给定一个随机变量Z，满足分布p(z)，则它的熵为：
$$
H(Z)=-\sum_{z\in C}p(z)\log_2p(z)
$$
其中，C是随机变量Z的可能取值的集合。分布越平坦，不确定性越大，熵也就越高。

此外还有条件熵的概念，即给定一个属性的属性值后另一个属性含有的熵。条件熵$H(ZIY)$定义为随机变量Z在知道随机变量Y的值后的熵：
$$
H(Z|Y=y)=-\sum_{z\in C}p(z|y)\log p(z|y)
$$

$$
H(Z|Y)=\sum_{y\in T}p(Y=y)H(Z|Y=y)
$$

给定属性时，已知信息不可能减少。因此同一个变量的条件熵一定小于等于其本身的熵，即恒有：
$$
H(Z)\geq H(Z|Y)
$$
在确定一个属性后，因为获得了更多的信息，从而信息熵减小了。信息熵的差值被称为信息增益。如给定属性Y后属性Z的信息增益为：
$$
IG(Y)=H(Z)-H(Z|Y)
$$

### 8.3 决策树学习

预测结果的熵定义为可能性。如P(结果为真)=2/3，P(结果为假)=1/3，可以算出来结果的熵为0.91。决策树选择拓展属性时，需要比较在各个属性的条件下，结果的条件熵，从而算出各个属性对结果的信息增益，从而得出获得信息最多的属性，作为拓展属性。

当树的每个叶子节点都能对训练样本进行正确分类时，即可停止拓展，而不需要拓展所有的属性。如果数据不能被完全分类，则全部属性拓展完时停止拓展。此外还可以设置人为标准，如树的最大深度、信息增益阈值作为拓展停止条件。

如果树太大或太复杂，它在训练数据上工作得很好，但在测试数据上可能表现不佳（过拟合）。为了控制决策树的复杂性，我们可以在学习决策树的同时对其进行剪枝，或者在完成生成决策树之后再对其进行剪枝。基于验证集的剪枝标准为：

- 剪去不会增加结果精确度的节点
- 贪婪地删除最能提高验证准确性的节点
- 当验证集的精度开始下降时停止剪枝

-----------

## 九、 bagging和boosting

### 9.1 集成学习介绍

学习一个总是能正确分类实例的强分类器是很困难的，但是学习很多弱分类器很简单。弱分类器可能在整个数据集上表现不佳，但可能在部分数据集上表现良好。如果弱分类器在输入空间的不同部分表现良好，则可以通过组合这些弱分类器来获得强分类器。那么我们需要考虑如何得到这些弱分类器并将它们组合起来。

两种组合方法：

- 不加权平均或多数投票
- 加权平均：给更好的分类器更高的权重

这里的弱分类器可以是不同种类的，如决策树、支持向量机、神经网络、逻辑回归等。

### 9.2 引导聚集算法bagging

bagging全称bootstrap aggregating，即引导聚集算法。

我们不知道如何在输入空间上得到在不同部分性能良好的分类器，因此考虑创建尽可能多样化的分类器，使得它们互不相关。比如：

1. 使用bootstrapping算法得到训练集的多个子集
   - 从N个样本的训练数据集中随机抽取N'个样本
   - 重复上述步骤K次得到K个子集$S_1,...,S_K$。因为是随机抽取，子集可能有重复部分。但这样保证了用各个子集训练的模型是无关的
2. 对每个子集训练一棵决策树
3. 使用大多数投票的方法合并K棵决策树

如果各个分类器是强相关的，通过多数投票将它们结合在一起对性能改进没有多大帮助。为了尽可能不相关，考虑在决策树的学习过程中加入额外的随机性。如在构建决策树时，只使用随机选择属性的子集，从而构建随机森林。

### 9.3 提升算法boosting

重复下面的步骤来提升弱分类器：

1. 找出错误分类的样本
2. 提升这些样本的权重，并重新训练分类器

最终将各分类器的预测结果进行加权平均。如何对实例和预测结果进行加权是关键。

针对二分类的AdaBoost算法（正负类为$\pm1$）：**注意下面的权重！样本权重和分类器的权重不是一个东西！**设$\epsilon$为某个分类器的准确率，则它的权重为$\alpha=\frac12\ln(\frac{1-\epsilon}{\epsilon})$，从而分类器权重与分类器的性能成正比。通过$e^\alpha$可以增加误分类的样本的权重，用$e^{-\alpha}$减少正确分类样本的权重。然后重复上述步骤进行迭代。每一次迭代都能得到不同权重的分类器。最终组合这些分类器，进行加权平均即可。形式化描述如下：

1. 初始化每个样本的权重$w_0^{(n)}=\frac1N,n=1,...,N$（即最初各个样本权重相同）
2. 在第k次迭代时，使用权重$w_{k-1}^{(n)}$训练得到分类器$h_k(\bold x)$
3. 计算加权准确率：

$$
\epsilon_k=\frac{\sum^N_{n=1}w_{k-1}^{(n)}I(y_i\neq h_k(\bold x_n))}{\sum^N_{n=1}w^{(n))}_{k-1}}
$$

4. 计算分类器的投票权重：

$$
a_k=\frac12\ln(\frac{1-\epsilon_k}{\epsilon_k})
$$

5. 更新权重：

$$
w_k^{(n)}=w_{k-1}^{(n)}\exp\{-y_ih_k(\bold x_i)\alpha_k\}
$$

AdaBosst使用的是指数损失：
$$
L=\sum^N_{n=1}\exp\{-y^{(n)}h_{combine}(\bold x^{(n)})\}
$$
其中h函数表示样本的加权求和，即线性分类器。AdaBoost算法等价于最小化指数损失。

-------------

## 十、 线性降维：主成成分分析PCA

### 10.1 动机

很多类型的数据的维数都是很高的，例如图片。如果我们直接处理原始数据，后续任务的复杂性可能会非常高，因此考虑降维。高维数据通常近似存在于一个低维的内在空间中。因此考虑寻找一个方向，使在新方向下表示的数据的维数显著减少。

### 10.2 考虑最小化重构误差

高维空间的正交方向（标准正交基）一组$\bold u$满足：
$$
\bold u^T_i\bold u_j=\delta_{ij}
$$
其中$i=j$时$\delta_{ij}=1$，否则为0。在给定的M个正交方向下，对样本$\bold x$的最佳近似为：
$$
\tilde x=\alpha_1\bold u_1+...+\alpha_M\bold u_M
$$
其中：
$$
\alpha_i=\bold u_i^T\bold x
$$
有：
$$
||\bold x-\tilde x||^2=||\bold x||^2-2\sum^M_{i=1}\alpha_i\bold u_i^T\bold x+\sum^M_{i=1}\alpha_i^2
$$
在$\alpha_i=\bold u_i^T\bold x$时，上述函数最小化，为0。因此，我们找到了一组$\alpha_i$用来表示原来的样本。但是哪个方向是最好的还是未知的。需要考虑在样本集下，使用什么标准正交基才能使得原始数据得到更好的表示。

首先预处理数据，将数据的样本中心移动到原点，即用$\bold x^{(n)}-\overline x$表示原来的样本。如此一来，误差表示为：
$$
E=\frac1N\sum^N_{n=1}||(\bold x^{(n)}-\overline x)-\tilde x^{(n)}||^2
$$
最终可以得到：
$$
E=||\bold X-\overline X||^2_F-\sum^M_{i=1}\bold u_i^T\bold S\bold u_i
$$
其中，
$$
\bold S=\frac1N\sum^N_{n=1}(\bold x^{(n)}-\overline x)(\bold x^{(n)}-\overline x)^T\\
\bold X\triangleq [\bold x^{(1)},\bold x^{(2)},...,\bold x^{(N)}]
$$
$||\cdot||_F$表示Frobenius范数，即矩阵的每一项的平方和最后再开根号。最小化损失，可以得到，$\bold u_i$为S的最大正交向量。

最终结论为，选择的$\bold u$为样本的协方差矩阵的最大的几个特征向量（最大意思是对应的特征值最大）。如果选择全部的特征向量作为方向，则不会有损失。要降维的话则从小到大去掉特征向量。

### 10.3 考虑最大化方差

最大化方差等价于尽可能保留原始数据的信息。经验证发现，在这种思想下得到的$\bold u$也是样本协方差矩阵的最大的多个特征向量。

### 10.4 SVD

奇异值分解Singular Value Decomposition 。

对于任何M*N的矩阵A，都能分解为：
$$
\bold A=\bold U\bold \Sigma\bold V^T
$$
其中$\bold U=[\bold u_1,...,\bold u_M]$，$\bold V=[\bold v_1,...,\bold v_N]$。满足：$\bold u_i$和$\bold v_i$是$\bold A\bold A^T$和$\bold A^T\bold A$的第$i$个特征向量，$\bold u^T_i\bold u_j=\bold v_i^T\bold v_j=\delta_{ij}$。$\bold \Sigma$是对角矩阵，对角线上的值为$\bold A\bold A^T$和$\bold A^T\bold A$的特征值开根号，称为奇异值，按降序排列。因为它是对角矩阵，所以：
$$
\bold A=\bold U\bold \Sigma\bold V^T=\sum^r_{i=1}\Sigma_{ii}\bold u_i\bold v_i^T
$$
其中r是该对角矩阵的对角线元素个数。

如果将样本中心化$\tilde X=[\bold x^{(1)}-\overline x,\bold x^{(N)}-\overline x]$则有：$\tilde X\tilde X^T=NS$，它和矩阵S含有相同特征向量。因此，对$\tilde X$做SVD，可以得到样本的主要方向。

### 10.5 PCA的其他应用

对图像进行降维后还原，可以对图像进行压缩与去噪。

----------

## 十一、 K平均聚类

### 11. 1 聚类介绍

给定一组数据集，聚类算法将它们分成不同的子组。我们希望类内实例高度相似，类间实例低相似。不同的相似性标准会导致不同的结果。

### 11.2 K平均

在样本集中，随机选取K个点作为中心$\bold \mu_k$，计算每个样本到中心点的距离，并将样本划分到离它最近的那个点的集群中。使用变量$r_{nk}$表示数据样本$\bold x^{(n)}$是否属于集群k：
$$
r_{nk}=\left\{\begin{matrix}1,k=arg\min_j||\bold x^{(n)}-\mu_j||^2\\0,otherwise\end{matrix}\right.
$$
对于每个集群，用所有样本的平均位置更新中心点的位置：
$$
\mu_k=\frac{\sum^N_{n=1}r_{nk}\bold x_n}{\sum^N_{n=1}r_{nk}}
$$
重复上面的样本分配和中心更新过程。该过程是保证收敛的。

如何选择K呢？类内距离之和会随着K的增大而减小，因此选择K的时候考虑寻找使得距离基本收敛的K值或是依据下游应用需求选择K值。

K平均聚类的结果高度依赖于初始化的中心点。选择初始化中心点的一般方法：

- 随机选择：随机选择数据样本作为初始中心。问题在于有可能选择了距离很近的多个样本作为初始中心
- 基于距离的方法：一开始随机选一个样本作为中心点，其他时候选择离各个中心最远的样本作为中心点。问题在于可能选到异常值
- 随机+距离方法：第一个中心点为随机选择的样本，从全部远离现有中心的样本中随机选择下一个中心

一个点要么属于一个集群，要么不属于，存在赋值难的问题。考虑软K平均：
$$
r_{nk}=\frac{e^{-\beta}||\bold x^{(n)}-\mu_k||^2}{\sum^K_{i=1}e^{-\beta}||\bold x^{(n)}-\mu_i||^2}
$$
这样一来，$r_{nk}$可以理解为样本属于某个集群的几率。

该算法还对异常值敏感。而且欧几里得距离决定了决策边界是球形的，当不同簇的形状不规则时，性能可能不佳。

---------

## 十二、 潜变量模型及其高斯情况

### 12.1 潜变量模型LVM

在监督学习中，回归和分类都可以理解为学习的条件概率分布。类似地，从概率建模的角度来看，无监督学习可以理解为对输入数据的概率分布进行学习，最简单的方法是假设该分布为高斯形式。但是一般情况下，样本的分布比高斯分布复杂得多。为了更好地建模，就需要更好的分布表示方法，即使用复杂的分布，但是训练成本又会增加。

因此，一个直观的思路是使用很多简单模型的加和表示复杂模型：
$$
p(\bold x)=\pi_1p_1(\bold x)+\pi_Kp_K(\bold x)
$$
其中，所有$\pi$的和为1，用来使得分布合法。而每个$p_i$表示简单的分布。上面的每一项又可以看作是联合分布的边缘分布：
$$
p_z(\bold x)\pi_z=p(\bold x|z)p(z)=p(\bold x,z)
$$
在联合分布$p(\bold x,\bold z)$中，只有$\bold x$是我们关注的，而$\bold z$为妨碍变量nuisance variable，在概率模型学中称为潜变量latent variable。这种模型被称为潜变量模型LVM。

LVM的优点：

- 强大的建模能力：将简单分布组合成复杂分布
- 良好的可解释性：潜变量可以看做是不同的域
- 表示学习：给定特定数据的潜在变量的值可以理解为原始数据的抽象表示

### 12.2 高斯潜变量模型

假设先验分布和条件分布都是高斯分布：

- $p(\bold z)=N(\bold z;\bold 0,\bold I)$
- $p(\bold x|\bold z)=N(\bold x;\bold W\bold z+\bold \mu,\sigma^2\bold I)$

联合分布为二者的乘积，训练目标是最大化边缘概率$p(\bold x)$。对z积分可以得到：
$$
p(\bold x)=N(\bold x;\bold\mu,\bold W\bold W^T+\sigma^2\bold I)
$$
得到的最终结果为：
$$
\mu=\frac{\sum^N_{n=1}x_n}{N}\\
\bold W=\bold U(\bold \Lambda-\sigma^2\bold I)^{\frac12}
$$
其中$\bold U$包含样本协方差矩阵的最大的M个特征向量，$\bold \Lambda$是协方差矩阵最大的M个特征向量对应的特征值组成的对角矩阵。

### 12.3 与PCA的联系

高斯隐变量模型被称为概率主成分分析。当$\sigma=0$时，$\bold W$是没有标准化的变量的主成成分。不为0时仍然很接近主成成分。概率PCA能处理不完整的数据，能够更好地推广到更复杂的模型。当M较小时运算效率更快，因为不需要矩阵分解。

-------------

## 十三、 隐变量模型：高斯混合和其他情况

### 13.1 高斯混合分布

分布表达式：
$$
p(\bold x)=\sum^K_{k=1}\pi_kN(\bold x;\bold\mu_k,\bold\Sigma_k)
$$
其中，$K$表示高斯分布的个数，$\pi_k$表示第$k$个分布的权重。$\sum^K_{k=1}\pi_k=1$。$\bold \mu_k$和$\bold \Sigma_k$是第$k$个高斯分布的均值向量和协方差矩阵。

高斯混合分布可以看作是联合分布的边缘分布：
$$
p(\bold x)=\sum^K_{k=1}\pi_kN(\bold x;\bold\mu_k,\bold\Sigma_k)\to\\
p(\bold x,\bold z)=p(\bold x|\bold z)p(\bold z)\\
=[\Pi_{k=1}^K[N(\bold x;\bold\mu_k\bold\Sigma_k)]^{z_k}]\cdot Cat(\bold z;\bold \pi)
$$
其中$\bold z$是独热向量，$Cat(\bold z;\bold\pi)$表示分类分布，满足：
$$
Cat(\bold z=onehot_k;\bold\pi)=\pi_k
$$
从而有：
$$
p(\bold x,\bold z)=\Pi^K_{k=1}[\pi_kN(\bold x;\bold\mu_k\bold\Sigma_k)]^{z_k}\\
p(\bold x)=\sum^K_{k=1}\pi_kN(\bold x;\bold\mu_k\bold\Sigma_k)
$$
所以隐变量模型可以等效地表示高斯混合分布。

### 13.2 学习分布参数

给定一组训练数据$\{\bold x^{(n)}\}^N_{n=1}$，目的是学习分布参数：
$$
\{\pi_k\bold\mu_k,\bold\Sigma_k\}^K_{k=1}\triangleq \bold\theta
$$
所以联合分布可以写成：
$$
p(\bold x^{(1)},...,\bold x^{(N)})=\Pi^N_{n=1}\sum^K_{k=1}\pi_kN(\bold x^{(n)};\bold\mu_k\bold\Sigma_k)
$$
对于概率模型，训练目标是使对数似然函数最大化，即:，即：
$$
\log p(\bold x^{(1)},...,\bold x^{(N)})=\sum^N_{n=1}\log\sum^K_{k=1}\pi_kN(\bold x^{(n)};\bold\mu_k\bold\Sigma_k)
$$
为了优化这个函数，需要用参数对其求导。

定义：
$$
\gamma_k\triangleq\sum^K_{i=1}\pi_iN(\bold x;\bold\mu_i\bold\Sigma_i)
$$
则有：
$$
\frac{\part\log p(\bold x)}{\part\bold\mu_k}=\gamma_k\cdot\bold\Sigma^{-1}_k(\bold x-\bold\mu_k)\\
\frac{\part\log p(\bold x)}{\part\bold\Sigma_k}=-\frac12\cdot\gamma_k\bold\Sigma_k^{-1}(\bold I-\bold S\bold \Sigma_k^{-1})\\
\frac{\part\log p(\bold x)}{\part\bold\pi_k}=\frac{\gamma_k}{\pi_k}
$$
为了满足$\pi_k\geq0$且$\sum^K_{k=1}\pi_k=1$，通常将得到的$\pi_k$用softmax函数处理：
$$
\bold\pi=softmax(\bold a)
$$
从而学习参数$\bold a$而不是$\bold\pi$。于是这些参数可以用梯度下降法更新。

### 13.3 其他LVM的情况

隐马尔可夫模型、文本建模（潜在狄利克雷分配）、图像建模（SBN）

-------------

## 十四、EM算法

### 14.1 关心的问题

给定一个联合分布$p(\bold x,\bold z;\bold \theta)$，其中$\bold x$是观察变量，$\bold z$是隐变量，我们需要最大化似然函数，即：
$$
\bold\theta=\arg\max_\bold\theta\log p(\bold x;\bold \theta)
$$
其中,
$$
p(\bold x;\bold \theta)=\sum_zp(\bold x,\bold z;\bold \theta)
$$
也就是说已知的是联合分布但需要优化边际分布。

### 14.2 EM算法

算法分为E和M两步。E指评估预期，M指更新数据。

- E-step：$Q(\bold\theta;\bold\theta^{(t)})=E_{p(\bold z|\bold x;\bold\theta^{(t)})}[\log p(\bold x,\bold z;\bold \theta)]$
- M-step：$\bold\theta^{(t+1)}=\arg\max_\bold\theta Q(\bold \theta;\bold\theta^{(t)})$

关键在于：

- 后验分布$p(\bold z|\bold x;\bold\theta^{(t)})$
- 关于后验分布的联合分布的期望$\log p(\bold x,\bold z;\bold \theta)$
- 最大化

### 14.3 理论支持

重新表示对数似然：
$$
\log p(\bold x;\bold \theta)=\log\frac{p(\bold x,\bold z;\bold\theta)}{p(\bold x|\bold z;\bold\theta)}\\
=\sum_{\bold z}q(\bold z)\log\frac{p(\bold x,\bold z;\bold\theta)}{q(z)}+\sum_\bold z\frac{q(\bold z)}{p(\bold x|\bold z;\bold\theta)}\\
=L(q,\bold\theta)+KL(q||p(\bold x|\bold z;\bold\theta))\quad\forall\bold\theta,q(\bold z)
$$
其中，KL散度用来测量两个分布q和p之间的距离，定义为：
$$
KL(q||p)\triangleq\int q(z)\log\frac{q(z)}{p(z)}dz\geq0
$$
因此，对于任意的分布$q(\bold z)$来说，参数$\bold\theta^{(t)}$在第$t$次迭代时，恒有：
$$
\log p(\bold x;\bold \theta^{(t)})
=L(q,\bold\theta^{(t)})+KL(q||p(\bold x|\bold z;\bold\theta^{(t)}))
$$
不同的$q(\bold z)$的选择会产生不同的分解结果。当KL散度为0，即$q(\bold z)=p(\bold z|\bold x;\bold \theta^{(t)})$时，有：
$$
\log p(\bold x;\bold \theta^{(t+1)})\geq\log p(\bold x;\bold \theta^{(t)})
$$
恒成立。所以EM算法可以保证每一步似然函数是变大的。

从参数空间来看，EM算法的步骤为：

- 第$t$次迭代的E步骤：推导表达式$L(p(\bold z|\bold x;\bold \theta^{(t)}),\bold\theta)$并给出模型参数$\bold\theta^{(t)}$
- 第$t$次迭代的M步骤：计算最优值$\bold\theta^{(t+1)}=\arg\max_\bold\theta L(p(\bold z|\bold x;\bold \theta^{(t)}),\bold\theta)$
- 开始第$t+1$次迭代的E步骤。重复上述步骤直至收敛。

### 14.4 例子：训练高斯隐函数模型

概率PCA模型：

- 先验分布：$p(\bold z)=N(\bold z;\bold 0,\bold I)$
- 似然函数：$p(\bold x|\bold z)=N(\bold x;\bold W\bold z+\bold \mu,\sigma^2\bold I)$

- 目标：给定训练集$\bold x_n$，最大化$\log p(\bold x)$

#### 14.4.1 E-step

求：$Q(\theta;\theta^{(t)})=\sum^N_{n=1}E_{p(\bold z_n|\bold x_n;\bold\theta^{(t)})}[\log p(\bold x_n,\bold z_n;\bold \theta)]$

有：
$$
p(\bold x,\bold z;\bold \theta)=\frac1{(2\pi\sigma^2)^{D/2}}\exp(-\frac{||\bold x-\bold W\bold z-\bold\mu||^2}{2\sigma^2})\cdot\frac1{(2\pi)^{M/2}}\exp(-\frac{||\bold z||^2}{2})\\
\log p(\bold x,\bold z;\bold \theta)=-\frac D2\log{(2\pi\sigma^2)}-\frac M2\log(2\pi)--\frac{||\bold x-\bold W\bold z-\bold\mu||^2}{2\sigma^2}-\frac{||\bold z||^2}{2}
$$

#### 14.4.2 M-step

$Q$对$\bold W$求导，令导数为0，得到的参数值赋值给$\bold W$

--------

## 十五、 EM变体

广义EM、马尔可夫链蒙特卡罗EM

----------

## 十六、 深度生成模型

### 16.1 深度生成模型

生成模型是一种潜在变量模型：$p(\bold x,\bold z)=p(\bold x|\bold z)p(\bold z)$，为了提高建模能力，在z和x之间引入了深度神经网络，形成了联合pdf：
$$
p(\bold x,\bold z)=p(\bold x|T(\bold z))p(\bold z)
$$
其中$T$表示神经网络。与生成模型GM相比，深度生成模型DGM使条件概率分布依赖于神经网络。

### 16.2 在VB-EM框架下的学习

因为DGM是隐变量模型，所以可以用EM算法训练。但因为神经网络的存在，很难算出后验分布$p(\bold z|\bold x;\bold \theta^{(t)})$，因此考虑VB-EM算法，使用简单分布来近似后验分布。

### 16.3 朴素梯度估算

### 16.4 使用Re-parameterization技巧估算梯度

对于服从概率分布$q_\Phi(\bold z)=N(\bold z,\bold \lambda,diag(\bold \eta^2))$的样本$\bold z^{(i)}$，可以表示为：
$$
\bold z^{(i)}=\bold\lambda+\bold\eta\cdot\bold\epsilon^{(i)}
$$
其中$\epsilon$是来自标准高斯噪声的样本。用重新参数化方法绘制的样本包含了未知参数$\lambda$和$\eta$。如此一来能将随机性与模型参数分离，使其相互独立，从而有：
$$
\frac{\part E_{q_\Phi(z|x)[\ln p_\theta(x|z)]}}{\part\theta}\approx\frac{\part\tilde L(\theta,\Phi)}{\part\theta}
$$

$$
\frac{\part E_{q_\Phi(z|x)[\ln p_\theta(x|z)]}}{\part\Phi}\approx\frac{\part\tilde L(\theta,\Phi)}{\part\Phi}
$$

### 16.5 掩盖推理

将$\lambda$和$\eta$作为神经网络的输出，而不是学习二者。

-------

## 十七、 异常检测

### 17.1 介绍

目标：检测或发现与其他数据样本显著偏离的数据样本，又被称为离群点检测和新颖性检测。

异常不能被明确地描述，详尽地列举异常现象是不可能的。非正式地说，任何看起来与正常样本显著不同的样本都可以被视为异常。

点异常：单个数据点本身可以确定为异常或非异常。

### 17.2 基于距离的方法

基于距离的方法背后的想法是：异常是指距离其他实例很远的实例。可以以到第k个最近邻的距离计算离群值，等使用打分标准来确定一个实例是否异常。

该方法直观且可解释，但复杂度高且对k值敏感，很难找到一个好的距离度量，特别是高维数据，如图像。

### 17.3 基于密度检测的方法

基本思想：异常是位于低密度区域的实例。

基本步骤：

1. 从一组给定的正常数据实例中估计正常数据的概率密度分布
2. 给定新的实例，如果其密度小于一个阈值则认为它是异常的

定义密度的方法：

- 核密度估计KDE：$p(x)=\frac1{Nh}\sum^N_{i=1}K(\frac{x=x_i}h)$，其中$K$是核函数，如高斯核、三角核等。$h$是控制平滑度（带宽）的参数。对$h$敏感且不适用高维特征。
- 用已知密度函数拟合数据，即训练一个分部去拟合正常数据。模型表达能力有限且高维不适用。
- 降维+上面的密度学习：解决高维问题
- 深度生成模型：对于复杂数据，我们可以在正态数据实例上训练一个深度生成模型，从而近似学习正态数据的概率分布。

### 17.4 基于重构的检测方法

对一个自动编码器进行普通数据实例的培训

1. 将高维数据压缩成低维表示

2. 找到数据恢复方法

如果输入数据正常，可以很好地重建；如果输入数据异常，就不能很好地重建。

### 17.5 单类分类器方法

支持向量数据描述(SVDD)基本思想：找到最小的超球体(半径为r，中心为c)，可以包含所有的数据实例。然而，这个公式对异常值的存在非常敏感：
$$
\min_{r,c}r^2\quad s.t.||\Phi(x_i)-c||^2\leq r^2\quad\forall i=1,2,...,N
$$
为了使得模型更有弹性，可以加入非负松弛变量：
$$
\min_{r,c,\zeta_i}r^2+\frac1{vn}\sum^n_{i=1}\zeta_i\quad s.t.||\Phi(x_i)-c||^2\leq r^2+\zeta_i\quad\zeta_i>0\quad\forall i=1,2,...,N\quad
$$
其中$v\in(0,1]$，为超参数，用于控制球外实例的比例。$\Phi$是非线性映射。可以是核函数或深度神经网络。

### 17.6 评估指标

真假阴性阳性

POC可能的检测结果曲线：FP/总阴性-TP/总阳性 曲线

ROC下面积(AUROC)是评价正常点与异常点分离程度的良好标准

Precision-Recall曲线：精确查全率曲线，TP/总阳性 - TP/全部数据 曲线

-----

## 十八、 相似性搜索

### 18.1 动机

给定查询项q，从大量数据中找到与之最类似的。最简单的方法是将查询q与数据库中所有数据项逐一比较，然后检索相似度最高的数据项。复杂度随着数据的增大而线性增加，不具有可拓展性。考虑到互联网上的海量数据，这种朴素的方法根本不适用

### 18.2 LSH局部敏感哈希

哈希函数的目标:为每个数据项分配不同的二进制代码，输入上的微小变化可能导致完全不同的哈希码。地址可能会发生冲突，但概率很低。无论数据库大小如何，都可以有效地检索与查询完全匹配的项。三个步骤:

1. 预处理：依据哈希码将每个项赋给对应的桶
2. .查询哈希码：使用哈希函数获取查询项的哈希码
3. 检索：从对应的桶中检索项

在传统的哈希函数中，稍有不同的项会被分配到完全不同的桶中，在LSH中，类似的项目需要尽可能分配到相同或附近的地址。利用位置敏感的散列算法，我们可以快速地从海量数据中检索相似项。现在的问题是如何获得位置敏感的哈希函数，知道数据的相似性。

### 18.3 LSH例子：最小哈希

文档表示：表格中每一列表示一个文档，内容为0或1，表示相关单词的存在与否。

相似性度量：度量文档A和文档B之间的相似性：
$$
J(A,B)=\frac{|A\cap B|}{|A\cup B|}
$$
又被称为Jaccard index雅卡尔指数。

文档签名：为具有高Jaccard索引的文档分配相同或相似代码：

1. 随机排列文档矩阵的行索引，记为$\pi$
2. 对于每一列，记录置换表中第一次值为1的行索引$h_\pi(C)=\min_\pi\pi(C)$
3. 重复上述步骤，一般至少100次。将不同次的结果用行表示，竖直拼接。签名矩阵的相似性反映了文件的相似性。

将签名哈希：

1. 将签名矩阵分为b个频带，每个频带r行
2. 使用传统的哈希函数将列的频带哈希成二进制代码

如果两个文档相似，相应的波段也可能相同。通过散列，相似的文档更有可能被赋予相同的代码。但是如果对整个列进行散列，则几乎肯定会分配不同的代码。通过对每个带进行哈希，得到b个哈希函数。最后的哈希码可以通过连接不同频带的码来获得。

为了获得良好的检索性能，为参数r和b设置适当的值是很重要的。如果r太小，可以检索到很多不同的文档，但是如果r太大，可能找不到类似的文件。

相似度仅用常用词的比例来衡量。但是，没有常用词的句子有相似的意思也是很常见的。

### 18.4 数据独立的LSH

监督哈希基本思想:鼓励具有相同标签的数据输出相似的代码，不同标签的数据输出不同的代码。给定一组图像：$J=\{I_1,...,I_n\}$，可以构造一个$n\times n$的相似性矩阵$S$，$S_{ij}$在$I_i$和$I_j$标签相同时为1，否则为-1。

将S分解为：$S\approx \frac1q\bold H\cdot\bold H^T$，通过不等号左右两边相减求行列式最小化求得$\bold H$。$\bold H$的第i行可以被看做$I_i$的哈希码。但是如此一来，新图像的哈希码不能算出。我们可以训练一个神经网络来建模图像和哈希码$(I_i,h_i)$之间的关系。

-------

## 十九、 图片和文本表示(表征学习)

简单来说，表示学习就是映射每个数据实例$x_i$到空间中的向量$\R^d$。这种实例可以是图像、文本、视频、音频等。显然，映射不是唯一的。数据可以以无数种方式映射到向量上。我们希望尽可能多地保留原始数据语义信息的表示(向量)。如，通过用二维向量表示手写数字，好的表示应该有来自相同数字的图像，它们之间距离很近。

与原始数据相比，表示通常对下游应用程序更友好，可以更加紧凑舍的表示，且弃掉不相关的信息，保留重要的语义信息。

### 19.1 图像表示

有监督：训练图像分类器，最后一个卷积层输出的特征映射可以看作是输入图像的表示。除了卷积特征映射，全连接倒数第二层的输出有时也被用作表示。

没有标签：用辅助标记数据集(如ImageNet)训练CNN，来自新数据集的图像表示可以通过将图像经过预先训练的CNN获得。预先训练的CNN可以被视为一个特征提取器。

无监督（没有标签和辅助数据）：一种直接的方法是对给定的无标记数据训练一个自动编码器，然后使用编码器提取表示。使用生成方法，可以很容易地将各种先验知识强加到学习过程中。

### 19.2 文本表示

与图像不同，自然语言(文本数据)本质上不是以数字格式出现的。为了从文本数据中学习，我们需要做的第一件事是用计算机可以处理的形式表示文本数据。

- 独热表示、词袋、TFIDF
- `Word2Vec`表示：RNN
- 基于BERG的表示


# 模式识别

课程混沌。笔记混沌。建议不要参考。

## 一、 简介

输入数据，输出模式pattern，输入转换到输出由计算机完成。

获取数据->提取特征->进行学习->评价评估->实际应用

模式识别的困难：

- 语义鸿沟：人和计算机对数据的理解不一样
- 计算能力：储存和计算开销
- 数据获取：难以获取或难以标注

----------

## 二、 基础数学知识

不懂的概念自己去学相关课程内容。模式识别课程中只是简单回顾。

### 2.1 向量

- 向量内积（点积）：行向量乘以列向量，对应元素一一相乘后相加
- 向量长度：向量和自己的转置做内积后开根号。值为1则称为单位向量
- 向量正交（垂直）：点积值为0
- 向量间的夹角：两向量内积等于二者的长度之积再乘以cos夹角
- 向量x在y上的投影记为$proj_yx$，长度由夹角公式计算，方向和y相同
- 柯西-施瓦茨不等式

$$
(\sum^n_{k=1}a_kb_k)^2\leq(\sum^n_{k=1}a_k^2)(\sum^n_{k=1}b_k^2)
$$

当且仅当存在实数c使得对于任意的k都有$a_k=cb_k$，等号成立
$$
[\int^b_af(x)g(x)dx]^2\leq[\int^b_a[f(x)]^2dx][\int^b_a[g(x)]^2dx]
$$
当且仅当存在实数c使得任意的$x\in[a,b]$都有$f(x)=cg(x)$

### 2.2 矩阵

- 方阵：m和n的维度大小相同
- 对角阵：只有对角线有非0元素的方阵
- 单位阵：对角线全为1的对角阵，记为I或$I_n$，其中n表示维度大小
- 矩阵乘法（叉乘），懂得都懂。
- 矩阵的幂：多个矩阵相乘
- 矩阵的转置：懂得都懂。本章所有^T符号表示转置
- 对称矩阵：对任意i和j有$X_{ij}=X_{ji}$

- 行列式，记为|X|或det(X)，有：
  - \|X\|=\|X^T\|
  - \|XY\|=\|X\|\|Y\|
  - |λX| = λ^n * |X|，其中X为n*n方阵
- 方阵的逆矩阵，记为X^-1。有：
  - $XX^{-1}=X^{-1}X=I_n$
  - X可逆：|X|不为0，等价于(X^-1)^-1=X
  - (λX)^-1 = 1/λ * X^-1
  - 括号外的逆或转置放入括号内，括号内的矩阵顺序反过来。懂得都懂
- 方阵的特征值和特征向量：对于n*n矩阵A和向量x，若Ax=λx，则λ为A的特征值，x为A的特征向量。
  - n阶方阵有n个特征值（不同特征值可能相等）
  - 特征值之和等于对角线元素之和
  - 行列式等于所有特征值的乘积
- 方阵的迹：记为tr(A)，对角线元素之和。tr(AB)=tr(BA)
- 实对称矩阵：每个项都是实数的对称矩阵
  - 特征向量相互垂直
  - 若E为特征向量（列向量）横向排列组成的矩阵，E是n*n的，是满秩的，rank(E)=n
  - 谱分解：X为所有特征值\*特征向量\*特征向量转置之和。若特征向量长为1则E是正交矩阵
  - X=EΔ(E^T)，其中Δ是个对角矩阵，$Δ_{ii}=λ_i$
  - E(E^T)=(E^T)E=I
  - LU分解，Cholesky分解，QR分解...

- 正定与半正定：对称方阵A对任意不为0的向量x有(x^T)Ax的值都大于0则A为正定。大于等于0则为半正定。(x^T)Ax称为二次型。特征值全为正数则正定。正定矩阵的任意主子矩阵也是正定的
- 对矩阵或向量求导相当于各个位置单独求导，形成新的向量或矩阵；将向量或矩阵作为自变量对某个式子求导，相当于对应位置求导，形成新的向量或矩阵；向量x对向量a求导，结果为矩阵，其(i, j)位置为对原向量x的j位置对a的i位置求导

### 2.3 概率与统计

随机变量可以粗略的看成一个函数，而不是数学分析意义上的变量。

- 概率密度函数pdf
  - 离散：对于可数不相容的事件$x_1,x_2,...$有：$p(X=x_i)=c_i$，$\sum_ic_i=1$
  - 连续：考虑$X\in(-\infin,\infin)$，$\int^{+\infin}_{-\infin}p(x)dx=1$。
- 连续分布函数cdf：
  - $F(x)=P(X\leq x)=\int^x_{-\infin}p(x)dx$
  - $F(-\infin)=0\leq F(x)\leq F(+\infin)=1$
  - 单调递增
  - $P(x_1<x<x_2)=F(x_2)-F(x_1)=\int^{x_2}_{x_1}p(x)dx$
  - $p(x)=F'(x)$
- 联合分布$P(X=\bold x)$，$\int p(\bold x)d\bold x=1$
- 条件分布$P(X=x|Y=y)$
  - $p(x,y)=p(y)p(x|y)$
  - $p(x)=\int_yp(x,y)dy$
  - 如果$x=g(y)$则$p_Y(y)=p_X(x)|\frac{dx}{dy}|=p_X(g(y))|g'(y)|$

- 多维分布的期望：假设函数f(x)在x服从分布p(x)时，有：$E[f(X)]=\sum_xf(x)\cdot p(X=x)$或者$E[f(X)]=\int f(x)p(x)dx$
  - 条件期望$E(f(x)|Y=y)=\sum_xf(x)\cdot p(x|y)$
  - 协方差$Var(X)=E[(X-EX)^2]=E[X^2]-(EX)^2$
  - 当p(x)和f(x)固定时期望和方差是定值

- 估计均值：对训练样本$x_1,x_2,...x_n$
  - 均值估计：$\overline x=\frac{1}{n}\sum^n_{i=1}x_i$
  - 协方差估计：$Cov(x)=\frac{1}{n}\sum^n_{i=1}(x_i=\overline x)(x_i-\overline x)^T$
  - 无偏估计$Cov(x)=\frac{1}{n-1}\sum^n_{i=1}(x_i=\overline x)(x_i-\overline x)^T$

- 独立与相关：如果对任意的x和y都有p(x,y)=p(x)p(y)则X和Y相互独立
  - $Cov(X,Y)=E[(X-EX)(Y-EY)]$
  - 相关系数$\rho_{XY}=\frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}\in[-1,1]$
  - $\rho_{XY}=0$则不相关，为正负一则完全相关。独立一定不相关，不相关不一定独立

- 高斯分布（正态分布）$N(\mu,\sigma^2)$：$p(x)=(2\pi)^{-\frac{1}{2}}(\sigma^2)^{-\frac{1}{2}}\exp\{-\frac{1}{2}(x-\mu)(\sigma^2)^{-1}(x-\mu)\}$
  - 若$X\geq0$，则$P(X\geq a)\leq\frac{EX}{a}$
  - $P((X-\mu)^2\geq k^2)\leq\frac{\sigma^2}{k^2}$
  - 正态分布中，不相关等价于独立

---------

## 三、 NN框架

### 3.1 最近邻规则

分类问题：

- 训练集$D={(x_1,y_1),(x_n,y_n),...,(x_n,y_n)}$，其中x为向量
- 样本分为C类
- 距离函数$d(x_1,x_2)$

给定一个测试样例x，发现其最近邻$i^*=argmin_id(x,x_i)$，输出预测值为$y_{i^*}$。

最近邻可能出现的问题：

- 出现平局：多个训练样本和测试样本的距离相同
- 出现离群点outlier: K-近邻解决（找K个最相近的邻居）

训练样本无限时，最近邻的错误率最多是最佳错误率的两倍。

计算和存储代价，假设使用欧氏距离：

- 欧氏距离计算的复杂度为O(d)，NN的复杂度为O(nd)，KNN的复杂度同样为O(nd)

- 近似最近邻ANN：不要求是距离最短的K个。第K个NN距离为$d_k$，则ANN要求选取所有距离满足$\hat d\leq (1+\epsilon)d_k$即可。可以将KNN的速度提高几个数量级
- 二值哈希：每个哈希函数把数据映射到0和1两个部分，m个哈希函数共同表示x，每个x用m位表示。计算和存储可以大幅简化，但必须设计好的哈希函数

### 3.2 系统各模块混合简介

#### 3.2.1 评价准则

机器学习。与领域无关的特征变换和抽取，针对不同的数据特点应用不同的算法

评价准则：泛化和测试误差：根本假设是分布满足(x,y)~p(x,y)

- 泛化误差：$E_{(x,y)~p(x,y)}[f(x)\neq y]$，通常无法实际计算
- 测试误差$err=\frac{1}{n}\sum^n_{i=1}bool(f(x_i)\neq y_i)$
- 准确率$acc=1-err$

一种常见的学习框架：代价最小化，求得f使得$min_f\frac{1}{n}\sum^n_{i=1}bool(f(x_i)\neq y_i)$。难以优化，不好求解。可以把不连续的指示函数换成性质相似但容易优化的函数，如$\sum_i(f(x_i)-y_i)^2$

过拟合、欠拟合与正则化

如果没有测试集：交叉验证（N倍交叉验证N-fold CV，通常N为5或10）：

1. 将训练集分成大小大致相等的N部分
2. 对于每一部分，将其作为测试集，其他部分为训练集，算得错误率
3. 交叉验证的错误率为上述错误率的平均值

#### 3.2.2 数据不平衡性

一类数据远远多于另一类。不平衡学习、代价敏感学习

- 准则一
  - 将数据分为TP、TN、FP、FN（真Ture/假False 阴Negative/阳Positive）
  - 总数TP+TN+FP+FN，正样本数P=TP+FN，负样本数N=FP+TN
  - 假阳率：FPR=FP/N
  - 假阴率：FNR=FN/P
  - 真阳率：TPR=TP/P
  - 准确率ACC=(TP+TN)/TOTAL
- 准则二：AUC-ROC: x轴为FPR，y轴为TPR（AUC表示所围面积）。单调递增

- 准则三：

  - 查准率precision: PRE=TP/(TP+FP)
  - 查全率Recall: REC=TP/P（和TPR一样）
  - 上述二者的调和平均harmonic mean：$(\frac{x^{-1}y^{-1}}{2})^{-1}=\frac{2xy}{x+y}$

  - F1=2TP/(2TP+FP+FN)

- 准则四：AUC-PR: x轴为查准率，y轴为查全率。不单调

#### 3.2.3 代价不平衡性

使用代价矩阵，如：
$$
\begin{bmatrix}
\lambda_{11} & \lambda_{12}\\
\lambda_{21} & \lambda_{22}
\end{bmatrix}=
\begin{bmatrix}
0 & 1\\
1 & 0 
\end{bmatrix}
$$
其中$\lambda_{ij}$表示真实值为i，预测值为j时的代价。0-1代价：正确时代价为0，否则为1。

代价的计算：$E_{(x,y)}[\lambda_{y,f(x)}]$。如果是0-1代价则和错误率一致。

#### 3.2.4 真实值

真实值大多是手工标注的，很多时候人也不知道确切答案。可能产生错误。

- 先验概率prior probability：$p(y=i)$
- 后验概率posterior probability：$p(y=i|x)$
- 类条件概率class conditional probability：$p(x|y=i)$
- 贝叶斯定理：$p(y=i|x)=\frac{p(x|y=i)p(y=i)}{p(x)}=$条件*先验/数据

- 贝叶斯决策规则：选择代价最小的类别输出$argmin_yE_{(x,y)}[\lambda_{y,f(x,y)}]$
- 贝叶斯防线：使用贝叶斯决策规则的风险。是理论上能得到的最好结果，记为R\*
- 0-1风险中，风险和错误等价。所以R\*是理论上的最小误差。1-R\*是理论上的最高准确率

#### 3.2.5 错误来源

真实但未知的函数F(x)只能由数据集D学习获得。回归的代价函数是欧几里得距离。下式称为**偏置方差分解**：
$$
E_D[(f_D(x)-F(x))^2]=(E_D[F(x)]-f_D(x))^2+E_D[(f_D(x)-E_D[f_D(x)])^2]
$$
简写为：
$$
E[(f-F)^2]=(F-Ef)^2+E[(f-Ef)^2]
$$

- $E[F-Ef]$为偏置bias。当训练集取样有差异时，其值不变。

- $E[(f-Ef)^2]=Var_D(f_D(x))$为方差，当训练集取样有差异时，其值改变。
- 误差=偏置平方+方差。当考虑到F(x)有白噪声，则误差=偏置平方+方差+噪声。估计误差时如果没有测试集则多次平均

偏置与数据无关，是由模型的复杂度决定的。如线性分类器的偏置大，七阶多项式的偏置小。但是方差和二者都有关系。噪音没有办法，只能寻求高质量的数据获取。

------

## 四、 主成分分析PCA

### 4.1 PCA基础

数据各维度之间不是相互独立的，内在维度通常远低于表面维度。因此需要降低数据维度。PCA是最常用的降维方法之一。

- 零阶表示zero-dimensional representation：寻找固定的m，使得：

$$
J_1(m)=\min_m\sum^n_{i=1}||x_i-m||^2
$$

​		最优解为：
$$
m^*=argmin_m\sum^n_{i=1}||x_i-m||^2=\frac{1}{n}\sum_{i=1}^nx_i
$$

- 一维表示：使用线性关系表示：$y_i=\bold w^T\bold x_i+b$，x为高维特征，y为新的一维特征

  优化目标函数：
  $$
  J_2(w)=\frac{1}{n}\sum^n_{i=1}||w^T(x_i-\overline x)||^2
  $$
  这样一来目标函数可以无穷大或无限小。解决方法：加上限制条件：w的模为1：
  $$
  argmax_w\frac{1}{n}\sum^n_{i=1}||(x_i-\overline x)^Tw||^2=w^Tcov(x)w
  $$
  

优化：拉格朗日乘子法：将有约束的优化问题转换为无约束的优化问题：
$$
f(w, \lambda)=w^TCov(x)w-\lambda(w^Tw-1)
$$
其中$\lambda$称为拉格朗日乘子。最优的必要条件：
$$
\frac{df}{dw}=2Cov(x)w-2\lambda w=0
$$

$$
\frac{df}{d\lambda}=0
$$

选取最大特征值$\lambda_1$对应的特征向量为$w_1$，则$\bold x \approx\bold{\overline x}+(w^T_1(\bold x-\bold{\overline x}))w_1$，所以$y_i=w^T_1(\bold x-\bold{\overline x})$。

- 降维之前的向量：$x_i$
- 降维之后的向量：$w_1^T(x_i-\overline x)w_1=y_iw_1$
- 原空间中重建的向量$\hat x=\overline x+y_iw_1$

如果需要更多投影方向，则需要$w_2,w_3,...$两两正交。有：
$$
x\approx\overline x+(w_1^T(x-\overline x))w_1+(w_2^T(x-\overline x))w_2+...
$$
重建：将$w_i$写成矩阵：$W=\begin{bmatrix}w_1 & w_2 & ... & w_d\end{bmatrix}$，则投影系数为$W^T(x-\overline x)$，投影方向为$W$。
$$
x=\overline x+WW^T(x-\overline x)
$$
重建是完全精确的，没有误差。

很多时候有些投影方向是噪声，需要扔掉一些方向。通常去掉特征值最小的那些，保持90%的能量：
$$
\frac{\lambda_1+\lambda_2+...+\lambda_T}{\lambda_1+\lambda_2+...+\lambda_d}>0.9
$$
寻找第一个T使得上面的等式不成立。现在有$\hat W = \begin{bmatrix}w_1 & w_2 & ... & w_T\end{bmatrix}(大小d\times T)$，则：
$$
x-\hat x=\sum^d_{j=T+1}(w^T_j(x-\overline x))w_j=\sum^d_{j=T+1}e_j
$$
有：

- 如果$j\neq k$则$e^T_je_k=0$
- $E(||x-\hat x||^2)=\sum^d_{j=T+1}E(||e_j||^2)$

- $E(||e_j||^2)=\lambda_j$

小结：PCA变换的步骤：

1. 训练样本$x_1,x_2,...,x_n$
2. 计算$\overline x$和$Cov(x)$
3. 求得$Cov(x)$的特征值和特征向量
4. 依据特征值选定T
5. 依据T确定矩阵$\hat W$

6. 对任何数据x，经过PCA变换后的特征为$y=\hat W^T(x-\overline x)$，重建则为$x\approx\hat x=\overline x+\hat Wy$

### 4.2 正态分布与PCA

若x服从D维的高斯分布$N(\mu,\Sigma)$，即：
$$
p(x)=(2\pi)^{-\frac{D}{2}}|\Sigma|^{-\frac{1}{2}}\exp\{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\}
$$
对x进行PCA操作后。

假设使用全部特征向量，则$y=W^T(x-\mu)$

$\Sigma=\sum^d_{i=1}\lambda_i\xi_i\xi_i^T=\sum^d_{i=1}\lambda_iw_iw_i^T=W\Lambda W^T$

其中$\Lambda$为一个对角矩阵，$\Lambda_{ii}=\lambda_i$

有：$y\sim N(0,\Lambda)$

PCA旋转了数据，使得新特征各个维度互不相关。注意，对于高斯分布，不相关意味着相互独立。

PCA减少了数据量，从而减少了计算量，缩短了训练、测试、识别时间，也减少了存储空间。**可能**去除了数据中的噪声，所以可能提高系统的识别精确度。

如果数据还服从高斯分布，完成PCA后，新特征各个维度互不相关、互相独立，有利于模式识别。

白化变换：
$$
y=(W\Lambda^{-\frac{1}{2}})^T(x-\mu)
$$
$y\sim N(0,I)$，具有各向同性。

------

## 五、 归一化

### 5.1 特征归一化

1. 每维度归一：不同的维度需要统一到相同的取值范围。如$x_{ij}=\frac{x_{ij}-x_{min,j}}{x_{max,j}-x_{min,j}}$。其中，$x_{ij}$表示训练样本i的第j维。这样能将数据统一到[0,1]。减去0.5再乘二可以统一到[-1,1]。
2. $l_2$或$l_1$归一化：若各个维度取值范围的不同是有意义的，不同数据点之间的大小（如向量长度）应该保持一致：$x_{ij}=\frac{x_{ij}}{||x_i||_{l_1}}$，其中$||x_i||_{l_1}=\sum_j|x_{ij}|$
3. zero-norm, unit variance ：如果每一个维度的数据都服从高斯分布，则可以将每一个维度归一化到N(0,1)。对于每一维度，计算均值和方差，有：$x_{ij}=\frac{x_{ij}-\hat\mu_j}{\hat\sigma_j}$

需要归一化测试数据时，不能从测试集找最大值、最小值、均值等。除了测试时，不要使用测试数据。测试集和训练集要使用相同的归一化方法，通过保存从训练集上取得的归一化参数，用同样的公式归一化测试集。

归一化的方法应该是根据数据的特点来选择的。归一化可能对准确度有极大的影响！不同的归一化方法可以混合使用。

### 5.2 Fisher线性判别分析（FLD）

标签将数据分为两类，分别有$N_1$和$N_2$个点。需要找一个投影方向 

---------

## 六、 支撑向量机SVM

在空间中有许多数据点，分为两类，标签分别记为1和-1。考虑用线性边界分开两类，假设能完美地线性分开数据集（即数据集线性可分）。有时可以有很多边界，在训练集上都100%正确。

### 6.1 边际margin

一个点（样例）的边际margin是其到分界超平面的垂直距离。SVM要求**最大化**所有训练样本的最小边际。有最小边际的点成为支撑向量support vector。

分类超平面可以表示为：$f(\bold x)=\bold w^T\bold x+b=0$。将不同的点x代入，如果结果等于零则在平面上，如果大于零则在超平面一边，小于零则在另一边。$\bold w$为其法向量。

边际的计算：假设$\bold x$的投影点为$\bold x_\perp$，则距离向量为$\bold x-\bold x_\perp$，其方向与$\bold w$相同，大小r可正可负，边际为其大小的绝对值。易求得：
$$
r=\frac{f(x)}{||\bold w||}
$$
边际大小为 r 的绝对值。

判断样例对错的方法：$y_if(\bold x_i)>0$则正确，否则错误。

### 6.2 问题的简化

SVM的形式化描述：
$$
\arg\max_{w,b}(\min_i(\frac{|\bold w^T\bold x_i+b|}{||\bold w||}))\\
\arg\max_{w,b}(\min_i(\frac{y_i(\bold w^T\bold x_i+b)}{||\bold w||}))\\
\arg\max_{w,b}(\frac{1}{||\bold w||}\min_i(y_i(\bold w^T\bold x_i+b)))\\
$$
对$\bold w$没有限制、对$y_if(\bold x_i)>0$只判断方向而不管大小。可以限定其值为1，则问题变为在限制$\min_i(y_i(\bold w^T\bold x_i+b))$时，最大化$\frac{1}{||w||}$。即：
$$
\arg\min_{w,b}\frac12\bold w^T\bold w
$$
使得$\forall i,y_i(\bold w^T\bold x_i+b)\geq1$。令：
$$
L(\bold w,b,\alpha)=\frac12\bold w^T\bold w-\sum_{i=1}^n\alpha_i(y_i(\bold w^T\bold x_i+b)-1)
$$
拉格朗日乘子法：
$$
\frac{\part L}{\part\bold w}=0\to\bold w=\sum^n_{i=1}\alpha_iy_i\bold x_i\\
\frac{\part L}{\part b}=0\to\sum^n_{i=1}\alpha_iy_i=0
$$
代入L得到：
$$
\tilde L(\bold \alpha)=\sum^n_{i=1}\alpha_i-\frac12\sum^n_{i=1}\sum^n_{j=1}\alpha_i\alpha_jy_iy_j\bold x_i^T\bold x_j
$$


原来的空间中变量为x，称为primal form。现在变为了$\alpha_i$，即拉格朗日乘子，称为对偶空间。SVM的对偶形式为：
$$
\arg\max_{\bold \alpha}\sum^n_{i=1}\alpha_i-\frac12\sum^n_{i=1}\sum^n_{j=1}\alpha_i\alpha_jy_iy_j\bold x_i^T\bold x_j
$$
使得：
$$
\alpha_i\geq0,\sum^n_{i=1}a_iy_i=0
$$

### 6.3 更复杂的情形

#### 6.3.1 soft margin

soft margin：可以允许少数点margin比 1 小，但是犯错误是有惩罚的：
$$
y_i(\bold w^T\bold x_i+b)\geq 1-\xi_i
$$
其中$\xi$为松弛变量slack variable，表示允许犯的错误。加上惩罚后问题形式变为：
$$
\arg\min_{\bold w,b,\bold\xi}\frac12\bold w^T\bold w+C\sum^n_{i=1}\xi_i
$$
使得：
$$
y_i(\bold w^T\bold x_i+b)\geq1-\xi_i,\xi_i\geq0
$$
对偶形式下仅依赖于内积。

#### 6.3.2 线性不可分

kernel trick：对于两个向量$\bold x,\bold y\in\R^d$以及非线性函数$K(\bold x,\bold y)$，对于满足某些条件的函数K，一定存在一个映射$\phi:\R^d\to\Phi$使得对于任意的$\bold x$和$\bold y$有：
$$
K(\bold x,\bold y)=\phi(\bold x)^T\phi(\bold y)
$$
非线性函数K表示两个向量的相似程度，等价于$\phi$里面的内积。$\Phi$称为特征空间，维度可以有限也可以无限。必须存在特征映射才能将非线性函数表示为特征空间中的内积。

Mercer条件（充分必要的条件）：对任何满足$\int g^2(\bold u)d\bold u<\infin$的非零函数，对称函数K满足条件：

$\int\int g(\bold u)K(\bold u,\bold v)g(\bold v)d\bold u\bold v\geq0$。等价于对任何一个样本集合$\{x_1,x_2,...,x_n\},x_i\in\R^d$，如果矩阵$K=[K_{i,j}]_{i,j}$总是半正定的，那么函数K满足Mercer条件。

于是这些条件下向量机表示为核支撑向量机Kernel SVM形式：
$$
\arg\max_\bold\alpha\sum^n_{i=1}\alpha_i-\frac12\sum^n_{i=1}\sum^n_{i=1}\alpha_i\alpha_jy_iy_jK(\bold x_i,\bold x_j)
$$
分类边界：
$$
\bold w=\sum^n_{i=1}\alpha_iy_i\phi(\bold x_i)
$$
预测方法：
$$
\bold w^T\phi(\bold x)=\phi(\bold x)^T(\sum^n_{i=1}\alpha_iy_i\phi(\bold x_i))=\sum^n_{i=1}\alpha_iy_iK(\bold x,\bold x_i)
$$
在线性可分时，
$$
\bold w=\sum^n_{i=1}\alpha_iy_i\bold x_i
$$

#### 6.3.3 多分类问题

将样本点分为C个类：1、2、3、...、C，设计$(^C_2)$个分类器，用 i 和 j ( i > j )两类的训练数据学习。每个测试样本都会得到C(C-1)/2个结果，然后投票。每个分类器采用二值输出，即输出 f 经过符号函数的结果。分类器是1对1的。

只设计C个分类器，1对多。第 i 个分类器用类 i 做正类，其他都为负类。最终找到最大的 f 对应的分类器 i 。

--------------

## 七、 概率

### 7.1 推理与决策

从统计学观点看，目的是得到映射$x\to y$。数据分布为$p(x)$，先验分布为$p(y)$，联合分布为$p(x,y)$，类条件分布为$p(x|y=i)$，后验分布为$p(y=i|x)$

假设PDF服从某种函数形式（如高斯函数），当指定其所有参数值后PDF就完全确定，不同的概率分布由不同的参数值确定，所以PDF就是估计参数。这种情况称为参数估计。

如果PDF不是任何已知形式的函数，考虑使用训练数据直接估计空间中任意点的密度。非参数不代表无参数，而是参数无限多。

- 生成generative模型：估计$p(x|y=i)$和$p(x)$然后用贝叶斯定理求$p(y=i|x)$
- 判别discriminative模型：直接估计$p(y=i|x)$

这些模型分为两个步骤：

- 推理inference：估计各种密度函数

- 决策decision：根据估计得到的PDF对任意的x给出输出

### 7.2 参数估计

#### 7.2.1 点估计

以高斯分布为例，假设$x~N(\mu,\sigma^2)$，数据集为$D=\{x_1,...,x_n\}$。参数记为$\bold \theta=(\mu,\sigma)$。考虑似然值：
$$
p(D|\bold\theta)=\Pi_ip(x_i|\theta)=\Pi_i\frac1{\sqrt{2\pi}\sigma}\exp(-\frac{(x-\mu)^2}{2\sigma^2})
$$
目前$\theta$不是随机变量所以$p(D|\theta)$不是条件分布。定义似然函数：
$$
l(\theta)=p(D|\theta)=\Pi_ip(x_i|\theta)
$$
取对数：
$$
ll(\theta)=\ln p(D|\theta)=\sum_i\ln p(x_i|\theta)
$$
最大似然估计：
$$
\theta^*=\arg\max_\theta ll(\theta)
$$
在高斯分布下，参数的似然估计MLE为：
$$
\mu^*=\frac1n\sum^n_{i=1}x_i\\
\sigma^*=\frac1n\sum^n_{i=1}(x_i-\mu^*)(x_i-\mu^*)^T
$$
最大后验估计MAP：
$$
\theta^*=\arg\max_\theta l(\theta)p(\theta)
$$
将参数自身不同取值的可能性$p(\theta)$考虑进来。

无信息验前时MLE等价于MAP。

#### 7.2.2 贝叶斯估计

MLE：视$\theta$为固定的参数，假设存在一个最佳的参数，目的是找到这个参数

MAP：将$p(\theta)$代入MLE，仍然假设存在最有的参数。

以上均称点估计。在贝叶斯观点中$\theta$是一个随机变量或分布，所以估计的是一个分布而不是点。$p(\theta|D)$是贝叶斯参数估计的输出，是一个完整的分布而不是一个点。

共轭先验：若$p(x|\theta)$存在先验$p(\theta)$使得$p(\theta|D)$和$p(\theta)$有相同的函数形式，从而简化推导和运算。理论上很完备，数学上很优美，但推导困难，计算量极大。

#### 7.2.3 KDE

常用的参数形式基本都是单模single modal的，不足以描述复杂的数据分布：即应该直接以训练数据自身来估计分布。

核密度估计KDE

--------

## 八、 度量与信息论

### 8.1 度量metric

数据表示为连续的实数值的向量$\bold x\in\R^d$。对于实数向量、可以计算距离或者相似程度的，成为metric data。此外还有标记数据：1，2，3分别表示苹果、梨子、香蕉（不能比较大小和相似性）；时间序列数据：不定向量等。

一个度量d必须满足，对任意向量x，y，z，有：

- 非负：$d(\bold x,\bold y)\geq0$
- 自反：$d(\bold x,\bold y)=0\iff \bold x=\bold y$
- 对称：$d(\bold x,\bold y)=d(\bold y,\bold x)$
- 三角不等式：$d(\bold x,\bold y)+d(\bold y,\bold z)\geq d(\bold x,\bold z)$

Lp距离：
$$
d_p(x,y)=(\sum^d_{i=1}|x_i-y_i|^p)^{\frac 1p}
$$
幂平均：
$$
M_p(\bold x)=(\frac 1n\sum^n_{i=1}x_i^p)^{\frac 1p}
$$

### 8.2 标记数据nominal data

比较数据：相同则为1，否则为0。即相似度为$bool(x=y)$。度量化：将标记数据1，2，3，...，m的每一个转换成一个向量，类似于one hot的做法。

### 8.3 信息论

用位作为信息的单位。

离散时，定义熵：
$$
H=-\sum^m_{i=1}P_i\log_2P_i
$$
定义$0\log_20=0$。

连续则为：
$$
h(x)=-\int p(x)\ln(p(x))dx
$$
虽有均值和方差固定的连续分布中，高斯分布具有最大的熵，或者说不确定性最大。

使用$I(X;Y)$表示X和Y共同的信息。
$$
I(x;Y)=H(X)-H(X|Y)=\sum_{x,y}p(x,y)\log_2\frac{p(x,y)}{p(x)p(y)}
$$
KL散度：对于两个离散分布P和Q：
$$
KL(P||Q)=\sum_ip_i\log_2\frac{P_i}{Q_i}
$$
KL大于零，等号成立当且仅当$\forall i,P_i=Q_i$。

$I(x;Y)=D_{KL}(p(x,y)||p(x)p(y))$

### 8.4 决策树

每次选择信息增益最大的属性进行分类。

------------

## 九、 稀疏数据和未对齐数据

稀疏指某种有效的表达方式上，高维稀疏的数据。

动态时间弯曲DTW：一个项目是一个时间序列， 包含若干顺序的数据，需要将数据对齐。

假设两组有顺序的数据$\bold x=(x_1,...x_n)$和$\bold y=(y_1,...,y_m)$，可能$m\neq n$。对于任意的$x_i$和$y_i$，存在函数$d(x_i,y_i)$来描述其距离。如果能找到好的匹配则可以计算 x 和 y 之间的距离。匹配的条件：

- 若$x_i$和$y_i$匹配，$d(x_i,y_i)$越小越好
- 匹配可以有跳过的数据，如$x_1$匹配$y_2$而不是$y_1$
- 匹配是按顺序的

选择总距离最小的匹配。为了处理不能匹配的数据，要给它们也进行匹配，即多对多的匹配。对所有的$x_i$，都要和一个$y_j$匹配，反之亦然。匹配的数据可以看做坐标。匹配问题编程了发现最佳路径。选择总距离最小的匹配：
$$
D(n,m)=\min_{r,t}\sum_{i=1}^{l(r,t)}d(x_{r_i},y_{t_i})
$$
其中$(\bold r,\bold t)=\{(r_i,t_i)\},i=1,2,...$是任意一条满足条件的路径，$l(\bold r,\bold t)$是它的长度。

求解时考虑拆分成若干个子问题。路径的起点总是(1,1)，记到(i,j)的最短距离为$D(i,j)$，则要求的是$D(n,m)$。如果只走一步到该点，则最后一步可横着、竖着、斜着走。则有：
$$
D(n,m)=d(n,m)+\min(D(n-1,m),D(n,m-1),D(n-1,m-1))
$$
变成动态规划问题。